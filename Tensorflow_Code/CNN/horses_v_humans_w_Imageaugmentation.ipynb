{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/CNN/horses_v_humans_w_Imageaugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37v_yExZppEp"
      },
      "source": [
        "# Ungraded Lab: Data Augmentation on the Horses or Humans Dataset\n",
        "\n",
        "In the previous lab, you saw how data augmentation helped improve the model's performance on unseen data. By tweaking the cat and dog training images, the model was able to learn features that are also representative of the validation data. However, applying data augmentation requires good understanding of your dataset. Simply transforming it randomly will not always yield good results.\n",
        "\n",
        "In the next cells, you will apply the same techniques to the `Horses or Humans` dataset and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lslf0vB3rQlU"
      },
      "outputs": [],
      "source": [
        "# Download the training set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kru1l6rzVtaF"
      },
      "outputs": [],
      "source": [
        "# Download the validation set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU6md1VHXGFo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "#Extract the training data\n",
        "zip_file = 'horse-or-human.zip'\n",
        "local_handler = zipfile.ZipFile(zip_file,'r')\n",
        "local_handler.extractall('./horse-or-human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRTUSe1dXegj"
      },
      "outputs": [],
      "source": [
        "#Extract the validation data\n",
        "zip_file = 'validation-horse-or-human.zip'\n",
        "local_handler = zipfile.ZipFile(zip_file,'r')\n",
        "local_handler.extractall('./validation-horse-or-human')\n",
        "\n",
        "#close the handler\n",
        "local_handler.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXQf9GaqXrgY"
      },
      "outputs": [],
      "source": [
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('horse-or-human','horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('horse-or-human','humans')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "validation_horse_dir = os.path.join('validation-horse-or-human','horses')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "validation_human_dir = os.path.join('validation-horse-or-human','humans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wK8t2FqgsXc"
      },
      "outputs": [],
      "source": [
        "print(len(os.listdir(train_horse_dir)))\n",
        "print(len(os.listdir(train_human_dir)))\n",
        "print(len(os.listdir(validation_horse_dir)))\n",
        "print(len(os.listdir(validation_human_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mUQ3wvYNgG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#Function to build the model\n",
        "def create_model():\n",
        "  model = keras.models.Sequential([\n",
        "      # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "      # This is the first convolution\n",
        "      keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(300,300,3)),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      # The second convolution\n",
        "      keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      # The third convolution\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      # The fourth convolution\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      # The fifth convolution\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      #Dropout layer,\n",
        "      keras.layers.Dropout(0.3),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      keras.layers.Flatten(),\n",
        "      # 512 neuron hidden layer\n",
        "      keras.layers.Dense(512, activation='relu'),\n",
        "      # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "      keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Set training parameters\n",
        "  model.compile(optimizer='adam', #works better with image augmentation\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2WSjlwqdJiE"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Data Augmentation - Training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True,\n",
        "    rescale=1.0/255.0\n",
        ")\n",
        "\n",
        "#Data Augmentation - Validation data\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    './horse-or-human',     # This is the source directory for training images\n",
        "    target_size=(300, 300), # All images will be resized to 300x300\n",
        "    class_mode='binary',    # Since we use binary_crossentropy loss, we need binary labels\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 32 using validation_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    './validation-horse-or-human',  # This is the source directory for training images\n",
        "    target_size=(300, 300),         # All images will be resized to 300x300\n",
        "    class_mode='binary',            # Since we use binary_crossentropy loss, we need binary labels\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFY-a6BPcjFo"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "#Initialize the constant EPOCH\n",
        "EPOCH = 30\n",
        "\n",
        "#Fit the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs = EPOCH,\n",
        "    verbose = 2,\n",
        "    validation_data = validation_generator,\n",
        "    # steps_per_epoch = 32,\n",
        "    # validation_steps = 8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "outputs": [],
      "source": [
        "# # Constant for epochs\n",
        "# EPOCHS = 20\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(\n",
        "#       train_generator,\n",
        "#       steps_per_epoch=8,\n",
        "#       epochs=EPOCHS,\n",
        "#       verbose=1,\n",
        "#       validation_data = validation_generator,\n",
        "#       validation_steps=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJsRu2WjB__"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.fugure_format = 'retina'\n",
        "\n",
        "def plot_loss_acc(history):\n",
        "  #-----------------------------------------------------------\n",
        "  # Retrieve a list of list results on training and test data\n",
        "  # sets for each training epoch\n",
        "  #-----------------------------------------------------------\n",
        "  acc      = history.history[     'accuracy' ]\n",
        "  val_acc  = history.history[ 'val_accuracy' ]\n",
        "  loss     = history.history[    'loss' ]\n",
        "  val_loss = history.history['val_loss' ]\n",
        "\n",
        "  epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "  #------------------------------------------------\n",
        "  # Plot training and validation accuracy per epoch\n",
        "  #------------------------------------------------\n",
        "  plt.plot  ( epochs,     acc, label='Training accuracy' )\n",
        "  plt.plot  ( epochs, val_acc, label='Validation accuracy' )\n",
        "  plt.title ('Training and validation accuracy')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.figure()\n",
        "\n",
        "  #------------------------------------------------\n",
        "  # Plot training and validation loss per epoch\n",
        "  #------------------------------------------------\n",
        "  plt.plot  ( epochs,     loss, label='Training loss' )\n",
        "  plt.plot  ( epochs, val_loss, label='Validation loss' )\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title ('Training and validation loss'   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN2FZ9udjJKI"
      },
      "outputs": [],
      "source": [
        "# Plot training results\n",
        "plot_loss_acc(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "h9Gr__bF1mYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DECKPHMT5Xpw"
      },
      "outputs": [],
      "source": [
        "no_of_gpu =len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "print(\"Total GPUS: \", no_of_gpu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RcvhrAI6lfd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "tf.test.gpu_device_name()\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyabYvCsvtn"
      },
      "source": [
        "As you can see in the results, the preprocessing techniques used in augmenting the data did not help much in the results. The validation accuracy is fluctuating and not trending up like the training accuracy. This might be because the additional training data generated still do not represent the features in the validation data. For example, some human or horse poses in the validation set cannot be mimicked by the image processing techniques that `ImageDataGenerator` provides. It might also be that the background of the training images are also learned so the white background of the validation set is throwing the model off even with cropping. Try looking at the validation images in the `tmp/validation-horse-or-human` directory (note: if you are using Colab, you can use the file explorer on the left to explore the images) and see if you can augment the training images to match its characteristics. If this is not possible, then at this point you can consider other techniques and you will see that in next week's lessons."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}