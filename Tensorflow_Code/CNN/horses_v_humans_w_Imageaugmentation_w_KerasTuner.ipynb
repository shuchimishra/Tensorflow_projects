{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/CNN/horses_v_humans_w_Imageaugmentation_w_KerasTuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37v_yExZppEp"
      },
      "source": [
        "# Data Augmentation on the Horses or Humans Dataset\n",
        "\n",
        "In the previous lab, you saw how data augmentation helped improve the model's performance on unseen data. By tweaking the cat and dog training images, the model was able to learn features that are also representative of the validation data. However, applying data augmentation requires good understanding of your dataset. Simply transforming it randomly will not always yield good results.\n",
        "\n",
        "In the next cells, you will apply the same techniques to the `Horses or Humans` dataset and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install keras-tuner library; uncomment if necessary\n",
        "!pip install keras-tuner -q"
      ],
      "metadata": {
        "id": "-qpcVQhqKdUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "import keras_tuner\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "-FzhjqrDO0K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lslf0vB3rQlU"
      },
      "outputs": [],
      "source": [
        "# Download the training set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kru1l6rzVtaF"
      },
      "outputs": [],
      "source": [
        "# Download the validation set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU6md1VHXGFo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "#Extract the training data\n",
        "zip_file = 'horse-or-human.zip'\n",
        "local_handler = zipfile.ZipFile(zip_file,'r')\n",
        "local_handler.extractall('./horse-or-human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRTUSe1dXegj"
      },
      "outputs": [],
      "source": [
        "#Extract the validation data\n",
        "zip_file = 'validation-horse-or-human.zip'\n",
        "local_handler = zipfile.ZipFile(zip_file,'r')\n",
        "local_handler.extractall('./validation-horse-or-human')\n",
        "\n",
        "#close the handler\n",
        "local_handler.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXQf9GaqXrgY"
      },
      "outputs": [],
      "source": [
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('horse-or-human','horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('horse-or-human','humans')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "validation_horse_dir = os.path.join('validation-horse-or-human','horses')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "validation_human_dir = os.path.join('validation-horse-or-human','humans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wK8t2FqgsXc"
      },
      "outputs": [],
      "source": [
        "print(len(os.listdir(train_horse_dir)))\n",
        "print(len(os.listdir(train_human_dir)))\n",
        "print(len(os.listdir(validation_horse_dir)))\n",
        "print(len(os.listdir(validation_human_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking size of image**"
      ],
      "metadata": {
        "id": "Hlx6UaF0KSGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the module\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "# loading the image\n",
        "img = PIL.Image.open(os.path.join(train_horse_dir, os.listdir(train_horse_dir)[9]))\n",
        "\n",
        "# fetching the dimensions\n",
        "wid, hgt = img.size\n",
        "#print the pixels of image\n",
        "print(wid, hgt)"
      ],
      "metadata": {
        "id": "piTlcBe_EjoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Data Augmentation**"
      ],
      "metadata": {
        "id": "wj-LseEEPZdC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2WSjlwqdJiE"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Data Augmentation - Training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True,\n",
        "    rescale=1.0/255.0\n",
        ")\n",
        "\n",
        "#Data Augmentation - Validation data\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    './horse-or-human',     # This is the source directory for training images\n",
        "    target_size=(300, 300), # All images will be resized to 300x300\n",
        "    class_mode='binary',    # Since we use binary_crossentropy loss, we need binary labels\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 32 using validation_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    './validation-horse-or-human',  # This is the source directory for training images\n",
        "    target_size=(300, 300),         # All images will be resized to 300x300\n",
        "    class_mode='binary',            # Since we use binary_crossentropy loss, we need binary labels\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Keras Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "zVAPpaOiKVG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "_eKXbtU2YUbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(300, 300, 3))\n",
        "  x = inputs\n",
        "\n",
        "  # First Layer\n",
        "  x = keras.layers.Conv2D(filters=hp.Choice(\"filters-Conv2d-1\", values=[16,32,64,128,256]),kernel_size=(3,3), activation=\"relu\")(x)\n",
        "\n",
        "  # Tune whether to use MaxPooling2D.\n",
        "  if hp.Boolean(\"MaxPooling2D-1\"):\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-1\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-1', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  #Second Layer\n",
        "  x = keras.layers.Conv2D(filters = hp.Choice(\"filters-Conv2d-2\", values=[16,32,64,128,256]),kernel_size=(3,3), activation=\"relu\")(x)\n",
        "\n",
        "  # Tune whether to use MaxPooling2D.\n",
        "  if hp.Boolean(\"MaxPooling2D-2\"):\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-2\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-2', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  #Third Layer\n",
        "  x = keras.layers.Conv2D(filters = hp.Choice(\"filters-Conv2d-3\", values=[16,32,64,128,256]),kernel_size=(3,3), activation=\"relu\")(x)\n",
        "\n",
        "  # Tune whether to use MaxPooling2D.\n",
        "  if hp.Boolean(\"MaxPooling2D-3\"):\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-3\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-3', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  #Fourth Layer\n",
        "  x = keras.layers.Conv2D(filters = hp.Choice(\"filters-Conv2d-4\", values=[16,32,64,128,256]),kernel_size=(3,3), activation=\"relu\")(x)\n",
        "\n",
        "  # Tune whether to use MaxPooling2D.\n",
        "  if hp.Boolean(\"MaxPooling2D-4\"):\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-4\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-4', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  #Fifth Layer\n",
        "  x = keras.layers.Conv2D(filters = hp.Choice(\"filters-Conv2d-5\", values=[16,32,64,128,256]),kernel_size=(3,3), activation=\"relu\")(x)\n",
        "\n",
        "  # Tune whether to use MaxPooling2D.\n",
        "  if hp.Boolean(\"MaxPooling2D-5\"):\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-5\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-5', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  #Mandatory Flatten layer\n",
        "  x = keras.layers.Flatten()(x)\n",
        "\n",
        "  # Tune whether to use dropout.\n",
        "  if hp.Boolean(\"dropout-6\"):\n",
        "    x = keras.layers.Dropout(rate=hp.Float('dropout-rate-6', min_value=0, max_value=1.0, step=0.1, sampling='linear'))(x)\n",
        "\n",
        "  # Dense layers\n",
        "  x = keras.layers.Dense(units=hp.Choice(\"filters-dense\", values=[16,32,64,128,256,512,1024]),activation=\"relu\")(x)\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  #Stitch the model\n",
        "  model_tune = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  # Select optimizer\n",
        "  optimizer=hp.Choice('optimizer', values=['adam', 'RMSprop', 'SGD'])\n",
        "\n",
        "  # Conditional for each optimizer\n",
        "  if optimizer == 'adam':\n",
        "    learning_rate = hp.Float('lrate', min_value=1e-8, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "  elif optimizer == 'RMSprop':\n",
        "    learning_rate = hp.Float('lrate', min_value=1e-8, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "  elif optimizer == 'SGD':\n",
        "    learning_rate = hp.Float('lrate', min_value=1e-8, max_value=1e-3, sampling='LOG')\n",
        "    momentum = hp.Float('momentum', min_value=0, max_value=1.0, step=0.1, sampling='linear')\n",
        "\n",
        "  #compile the model\n",
        "  model_tune.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # model_tune.summary()\n",
        "\n",
        "  return model_tune"
      ],
      "metadata": {
        "id": "aq1xrDafKhK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "id": "VLA96hdYOX1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.BayesianOptimization( #can be Hyperband, RandomSearch, or BayesianOptimization\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True,\n",
        "    directory=\"./Model-Tuner\",\n",
        "    project_name=\"KerasTuning\",\n",
        ")"
      ],
      "metadata": {
        "id": "6cabf8IsOa49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print summary of search space\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "ghbi_Zm9OsQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #just clean the sessio, recomendable if we execute some times the model.\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    # This callback saves the best model based in val_accuracy\n",
        "    MCP = keras.callbacks.ModelCheckpoint(filepath='bestmodel.h5', monitor='val_accuracy', mode='auto', save_best_only=True, save_weights_only=False, verbose=1)\n",
        "    RLP = keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=3, verbose=1, mode=\"auto\", min_lr=0.000000001)\n",
        "    ES = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', patience=7, verbose=1) #min_delta=1e-4,"
      ],
      "metadata": {
        "id": "2CXP4pqKTcxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "#creates a function that updates the learning rate based on the epoch number\n",
        "def lr_update(epoch, lr):\n",
        "    \"\"\"\n",
        "    For the first 5 epochs the learning rate will be 0.005.\n",
        "    From epoch 6 and on, the learning rate will be reduced 1% per epoch\n",
        "    \"\"\"\n",
        "    if epoch <= 5:\n",
        "        # return 0.005\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * 0.99\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_update)"
      ],
      "metadata": {
        "id": "pzOeEEP_PFAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "num_epochs = 30\n",
        "\n",
        "history = tuner.search(train_generator,\n",
        "          epochs=num_epochs, verbose=2,\n",
        "          validation_data=validation_generator,\n",
        "                       steps_per_epoch=17, validation_steps=8,callbacks=[lr_scheduler, MCP, keras.callbacks.TensorBoard(\"./tb_logs\")]) #ES\n",
        "                      #  callbacks=[ES, RLP, MCP, keras.callbacks.TensorBoard(\"./tb_logs\")])\n",
        "\n",
        "#trial 5\n",
        "# Epoch 14: val_accuracy improved from 0.67578 to 0.87109, saving model to bestmodel.h5\n",
        "# 17/17 - 25s - loss: 0.6426 - accuracy: 0.6534 - val_loss: 0.6852 - val_accuracy: 0.8711 - lr: 0.0092 - 25s/epoch - 1s/step\n",
        "# Epoch 15/30\n",
        "\n",
        "# Epoch 15: val_accuracy did not improve from 0.87109\n",
        "# 17/17 - 25s - loss: 0.6347 - accuracy: 0.6573 - val_loss: 0.6838 - val_accuracy: 0.8398 - lr: 0.0091 - 25s/epoch - 1s/step"
      ],
      "metadata": {
        "id": "S1-bbLIHOwET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **View logs on Tensorboard**"
      ],
      "metadata": {
        "id": "WOv1-7QJOlxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to see the results in Tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir ./tb_logs"
      ],
      "metadata": {
        "id": "ks0jEQeNOAk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GAJJRHi8Z-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Query the results**"
      ],
      "metadata": {
        "id": "X6dnsQXChdaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "best_model.summary()\n"
      ],
      "metadata": {
        "id": "6-eHAKP0hnQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return best 5 trials\n",
        "best_trials = tuner.oracle.get_best_trials(num_trials=10)\n",
        "\n",
        "for trial in best_trials:\n",
        "    print(\"**********Trail id: \", trial.trial_id)\n",
        "    trial.summary()\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "MWUuCewzhsaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After hyperparameter tuning, retrieve the best hyperparameters.\n",
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "ZrMhqRBfMowS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the model**"
      ],
      "metadata": {
        "id": "894tL72bhu0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(best_hp.get_config(), compact=True)"
      ],
      "metadata": {
        "id": "j6TkZgmF_Mj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "best_model.save('./best/best_model.h5')\n",
        "best_model.save('./best/best_model_new_version')"
      ],
      "metadata": {
        "id": "xA2nbf_vh6qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Override the best model\n",
        "load first K best models then we need to use tuner's get_best_models method as below"
      ],
      "metadata": {
        "id": "HjdH6f33M37G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will load 10 best hyper tuned models with the weights\n",
        "# corresponding to their best checkpoint (at the end of the best epoch of best trial).\n",
        "best_model_count = 10\n",
        "bo_tuner_best_models = tuner.get_best_models(num_models=best_model_count).expect_partial()"
      ],
      "metadata": {
        "id": "CiADg_DBM3OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can access a specific best model as below:"
      ],
      "metadata": {
        "id": "FWKCTwWnNgL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_id = 1\n",
        "override_model = bo_tuner_best_models[best_model_id]\n",
        "override_model.summary()"
      ],
      "metadata": {
        "id": "1MWedY9vNcNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is for querying the models trained during the search. For best performance, it is recommended to retrain your Model on the full dataset using the best hyperparameters found during search, which can be obtained using tuner.get_best_hyperparameters().\n",
        "\n",
        "The best model according to me is second best model"
      ],
      "metadata": {
        "id": "JONAqh9gNHNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_best_hyperparameters = tuner.get_best_hyperparameters(num_trials=10)\n",
        "best_hp = tuner_best_hyperparameters[1]\n",
        "model_override = tuner.hypermodel.build(best_hp)\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "5od2CjoqOuBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualize the model**"
      ],
      "metadata": {
        "id": "o8lBCYysiBtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install graphviz\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(best_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "4I8Z5zeQiGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrain the model(optional)**"
      ],
      "metadata": {
        "id": "JvVyiGcciG9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the best hp.\n",
        "# model = build_model(best_hps)\n",
        "model_override = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "num_epochs = 30\n",
        "# Fit with the entire dataset.\n",
        "history = model_override.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels),\n",
        "                       callbacks=[ES, MCP, RLP], verbose=2)"
      ],
      "metadata": {
        "id": "nvESEcDriNX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training results\n",
        "plot_loss_acc(history)"
      ],
      "metadata": {
        "id": "zbsHETu7iSY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_override.evaluate(testing_padded, testing_labels)"
      ],
      "metadata": {
        "id": "RkoBLQe2TphV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}