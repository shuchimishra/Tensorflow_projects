{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkkrSg6V4qXlEgrrK2BomJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/Timeseries/exam/Category5_daily_temperature_0.089_MAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDER9G_9QyiI",
        "outputId": "52371b5d-cb14-4336-bfe6-b26adb8ed9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "10/10 - 13s - loss: 0.2853 - mae: 0.2853 - val_loss: 0.1577 - val_mae: 0.1577 - lr: 0.0010 - 13s/epoch - 1s/step\n",
            "Epoch 2/500\n",
            "10/10 - 5s - loss: 0.1150 - mae: 0.1150 - val_loss: 0.0929 - val_mae: 0.0929 - lr: 0.0010 - 5s/epoch - 479ms/step\n",
            "Epoch 3/500\n",
            "10/10 - 6s - loss: 0.0986 - mae: 0.0986 - val_loss: 0.1034 - val_mae: 0.1034 - lr: 0.0010 - 6s/epoch - 603ms/step\n",
            "Epoch 4/500\n",
            "10/10 - 5s - loss: 0.0978 - mae: 0.0978 - val_loss: 0.0913 - val_mae: 0.0913 - lr: 0.0010 - 5s/epoch - 471ms/step\n",
            "Epoch 5/500\n",
            "10/10 - 5s - loss: 0.0936 - mae: 0.0936 - val_loss: 0.0893 - val_mae: 0.0893 - lr: 0.0010 - 5s/epoch - 549ms/step\n",
            "Epoch 6/500\n",
            "10/10 - 4s - loss: 0.0918 - mae: 0.0918 - val_loss: 0.0869 - val_mae: 0.0869 - lr: 0.0010 - 4s/epoch - 407ms/step\n",
            "Epoch 7/500\n",
            "10/10 - 6s - loss: 0.0911 - mae: 0.0911 - val_loss: 0.0862 - val_mae: 0.0862 - lr: 0.0010 - 6s/epoch - 613ms/step\n",
            "Epoch 8/500\n",
            "10/10 - 5s - loss: 0.0906 - mae: 0.0906 - val_loss: 0.0858 - val_mae: 0.0858 - lr: 0.0010 - 5s/epoch - 499ms/step\n",
            "Epoch 9/500\n",
            "10/10 - 5s - loss: 0.0902 - mae: 0.0902 - val_loss: 0.0854 - val_mae: 0.0854 - lr: 0.0010 - 5s/epoch - 471ms/step\n",
            "Epoch 10/500\n",
            "10/10 - 5s - loss: 0.0897 - mae: 0.0897 - val_loss: 0.0847 - val_mae: 0.0847 - lr: 0.0010 - 5s/epoch - 487ms/step\n",
            "Epoch 11/500\n",
            "10/10 - 7s - loss: 0.0890 - mae: 0.0890 - val_loss: 0.0839 - val_mae: 0.0839 - lr: 0.0010 - 7s/epoch - 706ms/step\n",
            "Epoch 12/500\n",
            "10/10 - 5s - loss: 0.0883 - mae: 0.0883 - val_loss: 0.0829 - val_mae: 0.0829 - lr: 0.0010 - 5s/epoch - 451ms/step\n",
            "Epoch 13/500\n",
            "10/10 - 6s - loss: 0.0874 - mae: 0.0874 - val_loss: 0.0820 - val_mae: 0.0820 - lr: 0.0010 - 6s/epoch - 635ms/step\n",
            "Epoch 14/500\n",
            "10/10 - 5s - loss: 0.0869 - mae: 0.0869 - val_loss: 0.0811 - val_mae: 0.0811 - lr: 0.0010 - 5s/epoch - 452ms/step\n",
            "Epoch 15/500\n",
            "10/10 - 5s - loss: 0.0860 - mae: 0.0860 - val_loss: 0.0804 - val_mae: 0.0804 - lr: 0.0010 - 5s/epoch - 531ms/step\n",
            "Epoch 16/500\n",
            "10/10 - 5s - loss: 0.0855 - mae: 0.0855 - val_loss: 0.0806 - val_mae: 0.0806 - lr: 0.0010 - 5s/epoch - 543ms/step\n",
            "Epoch 17/500\n",
            "10/10 - 4s - loss: 0.0861 - mae: 0.0861 - val_loss: 0.0805 - val_mae: 0.0805 - lr: 0.0010 - 4s/epoch - 418ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "10/10 - 6s - loss: 0.0858 - mae: 0.0858 - val_loss: 0.0810 - val_mae: 0.0810 - lr: 0.0010 - 6s/epoch - 577ms/step\n",
            "Epoch 19/500\n",
            "10/10 - 6s - loss: 0.0856 - mae: 0.0856 - val_loss: 0.0802 - val_mae: 0.0802 - lr: 1.0000e-04 - 6s/epoch - 562ms/step\n",
            "Epoch 20/500\n",
            "10/10 - 5s - loss: 0.0855 - mae: 0.0855 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-04 - 5s/epoch - 457ms/step\n",
            "Epoch 21/500\n",
            "10/10 - 6s - loss: 0.0854 - mae: 0.0854 - val_loss: 0.0801 - val_mae: 0.0801 - lr: 1.0000e-04 - 6s/epoch - 621ms/step\n",
            "Epoch 22/500\n",
            "10/10 - 5s - loss: 0.0854 - mae: 0.0854 - val_loss: 0.0802 - val_mae: 0.0802 - lr: 1.0000e-04 - 5s/epoch - 548ms/step\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "10/10 - 5s - loss: 0.0854 - mae: 0.0854 - val_loss: 0.0801 - val_mae: 0.0801 - lr: 1.0000e-04 - 5s/epoch - 532ms/step\n",
            "Epoch 24/500\n",
            "10/10 - 5s - loss: 0.0854 - mae: 0.0854 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-05 - 5s/epoch - 461ms/step\n",
            "Epoch 25/500\n",
            "10/10 - 6s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-05 - 6s/epoch - 613ms/step\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "10/10 - 5s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-05 - 5s/epoch - 487ms/step\n",
            "Epoch 27/500\n",
            "10/10 - 5s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-06 - 5s/epoch - 491ms/step\n",
            "Epoch 28/500\n",
            "10/10 - 5s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-06 - 5s/epoch - 462ms/step\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "10/10 - 6s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-06 - 6s/epoch - 591ms/step\n",
            "Epoch 30/500\n",
            "10/10 - 5s - loss: 0.0853 - mae: 0.0853 - val_loss: 0.0800 - val_mae: 0.0800 - lr: 1.0000e-07 - 5s/epoch - 547ms/step\n",
            "Epoch 30: early stopping\n",
            "14/14 [==============================] - 6s 317ms/step\n",
            "Result is : 0.08951998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================================\n",
        "# PROBLEM C5\n",
        "#\n",
        "# Build and train a neural network model using the Daily Min Temperature.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
        "#\n",
        "# Desired MAE < 0.19 on the normalized dataset.\n",
        "# ============================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import urllib\n",
        "from tensorflow import keras\n",
        "\n",
        "#Custom callback\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epochs, logs={}):\n",
        "    if logs['val_mae'] < 0.2:\n",
        "      print(\"Stopping training since val mae is less than 0.2\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "def model_forecast(model, series, window_size, batch_size):\n",
        "  series = tf.expand_dims(series, axis=-1)\n",
        "  ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "  ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "  ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "  forecast = model.predict(ds)\n",
        "  return forecast\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "def download_and_extract_data():\n",
        "  data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "  urllib.request.urlretrieve(data_url, 'daily-min-temperatures.csv')\n",
        "\n",
        "def get_data():\n",
        "  data_file = \"daily-min-temperatures.csv\"\n",
        "  time_step = []\n",
        "  temps = []\n",
        "\n",
        "  with open(data_file) as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    step = 0\n",
        "    for row in reader:\n",
        "      temps.append(float(row[1]))\n",
        "      time_step.append(step)\n",
        "      step = step + 1\n",
        "\n",
        "  series = np.array(temps)\n",
        "  time = np.array(time_step)\n",
        "  return time, series\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "\n",
        "  download_and_extract_data()\n",
        "  time, series = get_data()\n",
        "\n",
        "  # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "  min=np.min(series, axis=0)\n",
        "  max=np.max(series, axis=0)\n",
        "  series -= min\n",
        "  series /= max\n",
        "\n",
        "  # DO NOT CHANGE THIS CODE\n",
        "  split_time=2500\n",
        "\n",
        "  time_train = time[:split_time]\n",
        "  x_train = series[:split_time]\n",
        "  time_valid = time[split_time:]\n",
        "  x_valid = series[split_time:]\n",
        "\n",
        "  # DO NOT CHANGE THIS CODE\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(42)\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # DO NOT CHANGE THIS CODE\n",
        "  window_size = 64\n",
        "  batch_size = 256\n",
        "  shuffle_buffer_size = 1000\n",
        "\n",
        "  train_set=windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "  valid_set=windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "  model=tf.keras.models.Sequential([\n",
        "    # YOUR CODE HERE.\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, activation='relu', padding='causal',input_shape=[None, 1]),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "  # Code to train and compile the model\n",
        "  # Initialize learning rate\n",
        "  lr = 1e-3\n",
        "\n",
        "  # Initialize the optimizer\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=lr\n",
        "                                     #  ,clipnorm=1\n",
        "                                     ) #clipnorm to avoid nan (exploding gradient problem) # YOUR CODE HERE\n",
        "\n",
        "  # Set the training parameters\n",
        "  model.compile(loss=tf.keras.losses.mean_absolute_error, optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "  #callback\n",
        "  callback = MyCallback()\n",
        "  RLP = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\",patience=3, verbose=1,mode=\"auto\") #3\n",
        "  ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\",patience=5, verbose=1, mode=\"auto\", start_from_epoch=5)\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit(train_set, epochs=500, validation_data=valid_set, verbose = 2, callbacks=[ES,RLP]) # YOUR CODE HERE\n",
        "\n",
        "  # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "  rnn_forecast = model_forecast(model, series, window_size, batch_size)\n",
        "  rnn_forecast = rnn_forecast[split_time - window_size:-1, 0]\n",
        "  x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "  result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "  print(\"Result is :\", result) #0.090\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import csv\n",
        "# import urllib\n",
        "# from tensorflow import keras"
      ],
      "metadata": {
        "id": "x6cGs8nHWtqr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "#     series = tf.expand_dims(series, axis=-1)\n",
        "#     ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#     ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "#     ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "#     ds = ds.shuffle(shuffle_buffer)\n",
        "#     ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "#     return ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "6vqHcZ73Wy18"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def download_and_extract_data():\n",
        "#   data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "#   urllib.request.urlretrieve(data_url, 'daily-min-temperatures.csv')"
      ],
      "metadata": {
        "id": "lRb2l8qrW5H1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_data():\n",
        "#   data_file = \"daily-min-temperatures.csv\"\n",
        "#   time_step = []\n",
        "#   temps = []\n",
        "\n",
        "#   with open(data_file) as csvfile:\n",
        "#     reader = csv.reader(csvfile, delimiter=',')\n",
        "#     next(reader)\n",
        "#     step = 0\n",
        "#     for row in reader:\n",
        "#       temps.append(float(row[1]))\n",
        "#       time_step.append(step)\n",
        "#       step = step + 1\n",
        "\n",
        "#   series = np.array(temps)\n",
        "#   time = np.array(time_step)\n",
        "#   return time, series"
      ],
      "metadata": {
        "id": "YsgpR7KwXKMZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download_and_extract_data()\n",
        "# time, series = get_data()"
      ],
      "metadata": {
        "id": "qJfM92pSYZB5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "# min=np.min(series, axis=0)\n",
        "# max=np.max(series, axis=0)\n",
        "# series -= min\n",
        "# series /= max"
      ],
      "metadata": {
        "id": "-V63_4lWXsbg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # DO NOT CHANGE THIS CODE\n",
        "# split_time=2500\n",
        "\n",
        "# time_train = time[:split_time]\n",
        "# x_train = series[:split_time]\n",
        "# time_valid = time[split_time:]\n",
        "# x_valid = series[split_time:]"
      ],
      "metadata": {
        "id": "tUyF_927Ymx5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # DO NOT CHANGE THIS CODE\n",
        "# tf.keras.backend.clear_session()\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(42)"
      ],
      "metadata": {
        "id": "nw3vnBjjZLjx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # DO NOT CHANGE THIS CODE\n",
        "# window_size = 64\n",
        "# batch_size = 256\n",
        "# shuffle_buffer_size = 1000"
      ],
      "metadata": {
        "id": "pJ8LwQi_Yxd1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_set=windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "# valid_set=windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer_size)"
      ],
      "metadata": {
        "id": "sZy0kSCwZWPc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for x,y in train_set.take(1):\n",
        "#   print(x.shape, y.shape)\n",
        "#   for i in range(1):\n",
        "#     print(x[i])\n",
        "#     print(y[i])"
      ],
      "metadata": {
        "id": "8VliNjRoZfQD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model=tf.keras.models.Sequential([\n",
        "#     # YOUR CODE HERE.\n",
        "#     tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, activation='relu', padding='causal',input_shape=[None, 1]),\n",
        "#     tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "#     tf.keras.layers.LSTM(64),\n",
        "#     tf.keras.layers.Dense(100, activation='relu'),\n",
        "#     tf.keras.layers.Dense(10, activation='relu'),\n",
        "#     tf.keras.layers.Dense(1)\n",
        "#     ])"
      ],
      "metadata": {
        "id": "z6gbpRXNZjjE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Custom callback\n",
        "# class MyCallback(tf.keras.callbacks.Callback):\n",
        "#   def on_epoch_end(self, epochs, logs={}):\n",
        "#     if logs['val_mae'] < 0.2:\n",
        "#       print(\"Stopping training since val mae is less than 0.2\")\n",
        "#       self.model.stop_training = True"
      ],
      "metadata": {
        "id": "gtYz6SPDWrgZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code to train and compile the model\n",
        "# # Initialize learning rate\n",
        "# lr = 1e-3\n",
        "\n",
        "# # Initialize the optimizer\n",
        "# optimizer=tf.keras.optimizers.Adam(learning_rate=lr\n",
        "#                                    #  ,clipnorm=1\n",
        "#                                    ) #clipnorm to avoid nan (exploding gradient problem) # YOUR CODE HERE\n",
        "\n",
        "# # Set the training parameters\n",
        "# model.compile(loss=tf.keras.losses.mean_absolute_error, optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# #callback\n",
        "# callback = MyCallback()\n",
        "# RLP = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\",patience=3, verbose=1,mode=\"auto\") #3\n",
        "# ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\",patience=5, verbose=1, mode=\"auto\", start_from_epoch=5)\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(train_set, epochs=500, validation_data=valid_set, verbose = 2, callbacks=[ES,RLP]) # YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "NEw4sCrJbzu8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# # BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "# def model_forecast(model, series, window_size, batch_size):\n",
        "#   series = tf.expand_dims(series, axis=-1)\n",
        "#   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#   ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#   ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#   ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#   forecast = model.predict(ds)\n",
        "#   return forecast"
      ],
      "metadata": {
        "id": "QDz19Txxca1R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "# rnn_forecast = model_forecast(model, series, window_size, batch_size)\n",
        "# rnn_forecast = rnn_forecast[split_time - window_size:-1, 0]\n",
        "# x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "# result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "# print(\"Result is :\", result) #0.090"
      ],
      "metadata": {
        "id": "Ic_ERR1Ad_Zw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "# %config InlineBackend.fugure_format = 'retina'\n",
        "# def plot_loss_acc(history):\n",
        "#   #-----------------------------------------------------------\n",
        "#   # Retrieve a list of list results on training and test data\n",
        "#   # sets for each training epoch\n",
        "#   #-----------------------------------------------------------\n",
        "#   mae      = history.history[     'mae' ]\n",
        "#   val_mae  = history.history[ 'val_mae' ]\n",
        "#   loss     = history.history[    'loss' ]\n",
        "#   val_loss = history.history['val_loss' ]\n",
        "#   epochs   = range(len(mae)) # Get number of epochs\n",
        "#   #------------------------------------------------\n",
        "#   # Plot training and validation accuracy per epoch\n",
        "#   #------------------------------------------------\n",
        "#   plt.plot  ( epochs,     mae, label='Training mae' )\n",
        "#   plt.plot  ( epochs, val_mae, label='Validation mae' )\n",
        "#   plt.title ('Training and validation mae')\n",
        "#   plt.grid()\n",
        "#   plt.legend()\n",
        "#   plt.xlabel(\"Epochs\")\n",
        "#   plt.ylabel(\"MAE\")\n",
        "#   plt.figure()\n",
        "#   #------------------------------------------------\n",
        "#   # Plot training and validation loss per epoch\n",
        "#   #------------------------------------------------\n",
        "#   plt.plot  ( epochs,     loss, label='Training loss' )\n",
        "#   plt.plot  ( epochs, val_loss, label='Validation loss' )\n",
        "#   plt.grid()\n",
        "#   plt.legend()\n",
        "#   plt.xlabel(\"Epochs\")\n",
        "#   plt.ylabel(\"Loss\")\n",
        "#   plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "QzNa2NcAgqLJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # Plot training results\n",
        "# plot_loss_acc(history)"
      ],
      "metadata": {
        "id": "2lMIKeESg-ak"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}