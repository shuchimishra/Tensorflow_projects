{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNugn7YZmsw3PpP7hH75kRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/Timeseries/exam/Category5_weather_station_0.226_MAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-_Y7d6J5U7P",
        "outputId": "4897efeb-a148-4126-b377-42daca660991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 40, 256)           133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 40, 256)           394240    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                1040      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 602401 (2.30 MB)\n",
            "Trainable params: 602401 (2.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "37/37 - 25s - loss: 0.4437 - mae: 0.4437 - val_loss: 0.2824 - val_mae: 0.2824 - lr: 0.0010 - 25s/epoch - 677ms/step\n",
            "Epoch 2/500\n",
            "37/37 - 10s - loss: 0.3081 - mae: 0.3081 - val_loss: 0.2668 - val_mae: 0.2668 - lr: 0.0010 - 10s/epoch - 276ms/step\n",
            "Epoch 3/500\n",
            "37/37 - 8s - loss: 0.3077 - mae: 0.3077 - val_loss: 0.2738 - val_mae: 0.2738 - lr: 0.0010 - 8s/epoch - 223ms/step\n",
            "Epoch 4/500\n",
            "37/37 - 9s - loss: 0.2737 - mae: 0.2737 - val_loss: 0.2329 - val_mae: 0.2329 - lr: 0.0010 - 9s/epoch - 242ms/step\n",
            "Epoch 5/500\n",
            "37/37 - 10s - loss: 0.2726 - mae: 0.2726 - val_loss: 0.2538 - val_mae: 0.2538 - lr: 0.0010 - 10s/epoch - 277ms/step\n",
            "Epoch 6/500\n",
            "37/37 - 10s - loss: 0.2687 - mae: 0.2687 - val_loss: 0.2485 - val_mae: 0.2485 - lr: 0.0010 - 10s/epoch - 280ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "37/37 - 8s - loss: 0.2710 - mae: 0.2710 - val_loss: 0.2720 - val_mae: 0.2720 - lr: 0.0010 - 8s/epoch - 229ms/step\n",
            "Epoch 8/500\n",
            "37/37 - 10s - loss: 0.2441 - mae: 0.2441 - val_loss: 0.2331 - val_mae: 0.2331 - lr: 1.0000e-04 - 10s/epoch - 280ms/step\n",
            "Epoch 9/500\n",
            "37/37 - 10s - loss: 0.2504 - mae: 0.2504 - val_loss: 0.2043 - val_mae: 0.2043 - lr: 1.0000e-04 - 10s/epoch - 284ms/step\n",
            "Epoch 10/500\n",
            "37/37 - 8s - loss: 0.2428 - mae: 0.2428 - val_loss: 0.2090 - val_mae: 0.2090 - lr: 1.0000e-04 - 8s/epoch - 226ms/step\n",
            "Epoch 11/500\n",
            "37/37 - 12s - loss: 0.2345 - mae: 0.2345 - val_loss: 0.2096 - val_mae: 0.2096 - lr: 1.0000e-04 - 12s/epoch - 320ms/step\n",
            "Epoch 12/500\n",
            "37/37 - 10s - loss: 0.2416 - mae: 0.2416 - val_loss: 0.1981 - val_mae: 0.1981 - lr: 1.0000e-04 - 10s/epoch - 276ms/step\n",
            "Epoch 13/500\n",
            "37/37 - 9s - loss: 0.2443 - mae: 0.2443 - val_loss: 0.2144 - val_mae: 0.2144 - lr: 1.0000e-04 - 9s/epoch - 249ms/step\n",
            "Epoch 14/500\n",
            "37/37 - 8s - loss: 0.2412 - mae: 0.2412 - val_loss: 0.1965 - val_mae: 0.1965 - lr: 1.0000e-04 - 8s/epoch - 219ms/step\n",
            "Epoch 15/500\n",
            "37/37 - 10s - loss: 0.2352 - mae: 0.2352 - val_loss: 0.2153 - val_mae: 0.2153 - lr: 1.0000e-04 - 10s/epoch - 260ms/step\n",
            "Epoch 16/500\n",
            "37/37 - 11s - loss: 0.2456 - mae: 0.2456 - val_loss: 0.1932 - val_mae: 0.1932 - lr: 1.0000e-04 - 11s/epoch - 285ms/step\n",
            "Epoch 17/500\n",
            "37/37 - 10s - loss: 0.2298 - mae: 0.2298 - val_loss: 0.1957 - val_mae: 0.1957 - lr: 1.0000e-04 - 10s/epoch - 275ms/step\n",
            "Epoch 18/500\n",
            "37/37 - 8s - loss: 0.2327 - mae: 0.2327 - val_loss: 0.2007 - val_mae: 0.2007 - lr: 1.0000e-04 - 8s/epoch - 219ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "37/37 - 10s - loss: 0.2445 - mae: 0.2445 - val_loss: 0.2041 - val_mae: 0.2041 - lr: 1.0000e-04 - 10s/epoch - 272ms/step\n",
            "Epoch 20/500\n",
            "37/37 - 10s - loss: 0.2384 - mae: 0.2384 - val_loss: 0.1976 - val_mae: 0.1976 - lr: 1.0000e-05 - 10s/epoch - 270ms/step\n",
            "Epoch 21/500\n",
            "37/37 - 8s - loss: 0.2353 - mae: 0.2353 - val_loss: 0.2071 - val_mae: 0.2071 - lr: 1.0000e-05 - 8s/epoch - 215ms/step\n",
            "Epoch 21: early stopping\n",
            "40/40 [==============================] - 8s 140ms/step\n",
            "Result is : 0.22583144726088714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to\n",
        "# its difficulty. So your Category 1 question will score significantly less\n",
        "# than your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict weather time series.\n",
        "# Using a window of past 40 observations, train the model to\n",
        "# predict the next one observation.\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# This is a custom dataset created by Google for the purpose of this\n",
        "# examination.\n",
        "# The dataset consists of temperature values ordered by time.\n",
        "# =============================================================================\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in the following functions:\n",
        "# 1. windowed_dataset()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be [BATCH_SIZE, N_PAST = 40, 1], since the\n",
        "#    testing infrastructure expects a window of past N_PAST = 40 observations\n",
        "#    of the variable to predict the next observation of the variable.\n",
        "#\n",
        "# 2. Model output_shape must be [BATCH_SIZE, N_FEATURES = 1]\n",
        "#    Refer to the code to see the definitions for these constants.\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# HINT: Your neural network must have a validation MAE of approximately 0.3 or\n",
        "# less on the normalized dataset for top marks.\n",
        "\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow import keras\n",
        "\n",
        "#Custom callback\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epochs, logs={}):\n",
        "    if logs['val_mae'] < 0.3:\n",
        "      print(\"Stopping training since val mae is less than 0.3\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "def mae(y_true, y_pred):\n",
        "   return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
        "\n",
        "\n",
        "def model_forecast(model, series, window_size, batch_size):\n",
        "   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "   ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "   ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "   ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "   forecast = model.predict(ds)\n",
        "   return forecast\n",
        "\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/weather_station.zip'\n",
        "    urllib.request.urlretrieve(url, 'weather_station.zip')\n",
        "    with zipfile.ZipFile('weather_station.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "# This function is used to load the time series data from a\n",
        "# csv file \"station.csv\". Each line has 12 comma separated observations\n",
        "# corresponding to months in a year. The first line in the csv is the header\n",
        "# having names of columns(months).\n",
        "# The function reads the CSV line by line and appends observations for\n",
        "# each month in a year, to a 1D array named temperatures so as to record\n",
        "# monthly data for temperatures as the dataset.\n",
        "def get_data():\n",
        "    data_file = \"station.csv\"\n",
        "    f = open(data_file)\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "    lines = data.split('\\n')\n",
        "    header = lines[0].split(',')\n",
        "    lines = lines[1:]\n",
        "    temperatures = []\n",
        "    for line in lines:\n",
        "        if line:\n",
        "            linedata = line.split(',')\n",
        "            linedata = linedata[1:13]\n",
        "            for item in linedata:\n",
        "                if item:\n",
        "                    temperatures.append(float(item))\n",
        "\n",
        "    series = np.asarray(temperatures)\n",
        "    time = np.arange(len(temperatures), dtype=\"float32\")\n",
        "    return time, series\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and\n",
        "# validation. First element of the first window will be the first element of\n",
        "# the dataset. Consecutive windows are constructed by shifting\n",
        "# the starting position of the first window forward, one at a time (indicated\n",
        "# by shift=1). For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the end of the\n",
        "# window.\n",
        "\n",
        "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
        "def windowed_dataset(series, batch_size, n_past=40, n_future=1, shift=1):\n",
        "    # Adds an extra dimension of size 1 to the series.\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "\n",
        "    # This line converts the dataset into a windowed dataset where a\n",
        "    # window consists of both the observations to be included as\n",
        "    # features and the targets.\n",
        "    #\n",
        "    # DON'T change the shift parameter. The test windows are\n",
        "    # created with the specified shift and hence it might affect your\n",
        "    # scores. You must calculate the window size so that based on\n",
        "    # the past 40 observations (observations at time steps t=1,t=2,\n",
        "    # ...t=40) of the 1 variable in the dataset, you predict the next 1\n",
        "    # observation (observation at time step t=41) of the 1 variable in the\n",
        "    # dataset.\n",
        "\n",
        "    # Hint: Each window should include both the past observations and the\n",
        "    # future observations which are to be predicted. Calculate the window size\n",
        "    # based on n_past and n_future.\n",
        "    # Note: This line returns a Dataset of Datasets (each dataset holds\n",
        "    # elements of one window).\n",
        "    ds = ds.window(size= n_past + n_future,\n",
        "                    shift = shift,\n",
        "                    drop_remainder = True)\n",
        "\n",
        "    # This takes the Dataset of Datasets and flattens it into a single\n",
        "    # dataset of tensors.\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "\n",
        "    # Now, each element of the dataset is a tensor holding\n",
        "    # n_past + n_future observations corresponding to each window.\n",
        "    # This line maps each window of the dataset to the form\n",
        "    # (n_past observations, n_future observations) which is the input format\n",
        "    # needed for training the model.\n",
        "    # Hint: Use a lambda function to map each window in the dataset to its\n",
        "    # respective (features, targets).\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "\n",
        "    ds = ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "# This function downloads the dataset, loads the data from CSV file,\n",
        "# normalizes the data and splits the dataset into train and validation\n",
        "# sets. It also uses windowed_dataset() to split the data into\n",
        "# windows of observations and targets. (Refer to the function for more\n",
        "# information). Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Loads the data and reads it line by line to extract time ordered values\n",
        "    # of the single feature.\n",
        "    download_and_extract_data()\n",
        "    time, series = get_data()\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    mean = series.mean(axis=0)\n",
        "    series -= mean\n",
        "    std = series.std(axis=0)\n",
        "    series /= std\n",
        "\n",
        "    # The data is split into training and validation sets at SPLIT_TIME.\n",
        "    SPLIT_TIME = 780  # DO NOT CHANGE THIS CODE\n",
        "    x_train = series[:SPLIT_TIME]\n",
        "    x_valid = series[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # There is only one feature varying with time in this dataset.\n",
        "    # We predict one feature using past observations of same feature.\n",
        "    N_FEATURES = 1 # DO NOT CHANGE THIS CODE\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, setting it to higher sizes might affect your\n",
        "    # scores.\n",
        "    BATCH_SIZE = 20  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should\n",
        "    # be predicted\n",
        "    N_PAST = 40  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_dataset = windowed_dataset(series=x_train,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     n_past=N_PAST,\n",
        "                                     n_future=N_FUTURE)\n",
        "    valid_dataset = windowed_dataset(series=x_valid,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     n_past=N_PAST,\n",
        "                                     n_future=N_FUTURE)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (BATCH_SIZE,N_PAST=40,1)\n",
        "        # The model must have an output shape of (BATCH_SIZE, 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since model predicts one feature.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(N_PAST, N_FEATURES)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)), #this is important to have output shape of batch_size, N_FUTURE, N_FEATURES\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Dense(16),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(N_FEATURES)])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    # Initialize learning rate\n",
        "    lr = 1e-3\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr\n",
        "                                       #  ,clipnorm=1\n",
        "                                       ) #clipnorm to avoid nan (exploding gradient problem) # YOUR CODE HERE\n",
        "\n",
        "    # Set the training parameters\n",
        "    model.compile(loss=tf.keras.losses.mean_absolute_error, optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    #callback\n",
        "    callback = MyCallback()\n",
        "    RLP = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\",patience=3, verbose=1,mode=\"auto\") #3\n",
        "    ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\",patience=5, verbose=1, mode=\"auto\", start_from_epoch=5)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_dataset, epochs=500, validation_data=valid_dataset, verbose = 2, callbacks=[ES,RLP]) # YOUR CODE HERE\n",
        "\n",
        "    # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "    rnn_forecast = model_forecast(model, series[...,np.newaxis], N_PAST, BATCH_SIZE)\n",
        "    rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0]\n",
        "    x_valid = x_valid[:rnn_forecast.shape[0]]\n",
        "    result = mae(x_valid, rnn_forecast)\n",
        "    print(\"Result is :\", result) #0.2285\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import urllib"
      ],
      "metadata": {
        "id": "i2XOW83l6E6S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyCallback(tf.keras.callbacks.Callback):\n",
        "#   def on_epoch_end(self, epochs, logs={}):\n",
        "#     if logs['val_mae'] < 0.3:\n",
        "#       print(\"Stopping training since val mae is less than 0.3\")\n",
        "#       self.model.stop_training = True"
      ],
      "metadata": {
        "id": "7WcNY9N1B5MX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_data():\n",
        "#   data_file = \"station.csv\"\n",
        "#   f = open(data_file)\n",
        "#   data = f.read()\n",
        "#   f.close()\n",
        "#   lines = data.split('\\n')\n",
        "#   lines = lines[1:]\n",
        "#   temperatures = []\n",
        "#   for line in lines:\n",
        "#     if line:\n",
        "#       linedata = line.split(',')\n",
        "#       linedata = linedata[1:13]\n",
        "#       for item in linedata:\n",
        "#         if item:\n",
        "#           temperatures.append(float(item))\n",
        "\n",
        "#   series = np.asarray(temperatures)\n",
        "#   time = np.arange(len(temperatures), dtype=\"float32\")\n",
        "#   return time, series"
      ],
      "metadata": {
        "id": "eIJYtkid6MDW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def windowed_dataset(series, batch_size, n_past=40, n_future=1, shift=1):\n",
        "#   series = tf.expand_dims(series, axis=-1)\n",
        "#   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#   ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "#   ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "#   ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "#   ds = ds.batch(batch_size).prefetch(1)\n",
        "#   return ds"
      ],
      "metadata": {
        "id": "Tp-hB_866jMP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def download_and_extract_data():\n",
        "#   url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/weather_station.zip'\n",
        "#   urllib.request.urlretrieve(url, 'weather_station.zip')\n",
        "#   with zipfile.ZipFile('weather_station.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall()"
      ],
      "metadata": {
        "id": "dbu1GBem6G7-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download_and_extract_data()\n",
        "# time, series = get_data()"
      ],
      "metadata": {
        "id": "DUT4qtqm65Ij"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean = series.mean(axis=0)\n",
        "# series -= mean\n",
        "# std = series.std(axis=0)\n",
        "# series /= std"
      ],
      "metadata": {
        "id": "cB5DkW1h7AKm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPLIT_TIME = 780\n",
        "# x_train = series[:SPLIT_TIME]\n",
        "# x_valid = series[SPLIT_TIME:]"
      ],
      "metadata": {
        "id": "AiaNgRvx7CO6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(42)"
      ],
      "metadata": {
        "id": "371UnfYE7FU1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N_FEATURES = 1\n",
        "# BATCH_SIZE = 20\n",
        "# N_PAST = 40\n",
        "# N_FUTURE = 1\n",
        "# SHIFT = 1"
      ],
      "metadata": {
        "id": "xXkrBaHi7IB_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = windowed_dataset(series=x_train,batch_size=BATCH_SIZE,n_past=N_PAST,n_future=N_FUTURE)\n",
        "\n",
        "# valid_dataset = windowed_dataset(series=x_valid,batch_size=BATCH_SIZE,n_past=N_PAST,n_future=N_FUTURE)"
      ],
      "metadata": {
        "id": "CfEGe6DP7MwY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for x, y in train_dataset.take(1):\n",
        "#   print(x.shape)\n",
        "#   print(y.shape)\n",
        "#   for i in range(2):\n",
        "#     print(x[i])\n",
        "#     print(y[i])"
      ],
      "metadata": {
        "id": "uGG6-5P0_QPW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.models.Sequential([\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(N_PAST, N_FEATURES)),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)), #this is important to have output shape of batch_size, N_FUTURE, N_FEATURES\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "\n",
        "#     tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "#     tf.keras.layers.Dense(16),\n",
        "#     tf.keras.layers.Dropout(0.2),\n",
        "#     tf.keras.layers.Dense(N_FEATURES)])"
      ],
      "metadata": {
        "id": "69Y3adIBBL3x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "LM8sFsPzA2cE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize learning rate\n",
        "# lr = 1e-3\n",
        "\n",
        "# # Code to train and compile the model\n",
        "# # Initialize the optimizer\n",
        "# optimizer=tf.keras.optimizers.Adam(learning_rate=lr\n",
        "#                                   #  ,clipnorm=1\n",
        "#                                    ) #clipnorm to avoid nan (exploding gradient problem) # YOUR CODE HERE\n",
        "\n",
        "# # Set the training parameters\n",
        "# model.compile(loss=tf.keras.losses.mean_absolute_error, optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# #callback\n",
        "# callback = MyCallback()\n",
        "# RLP = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\",patience=3, verbose=1,mode=\"auto\") #3\n",
        "# ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\",patience=5, verbose=1, mode=\"auto\", start_from_epoch=5)\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(train_dataset, epochs=500, validation_data=valid_dataset, verbose = 2, callbacks=[ES,RLP]) #batch_size=32, # YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "2ZDUHpfKBk4e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# # BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "# def mae(y_true, y_pred):\n",
        "#    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
        "\n",
        "\n",
        "# def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast"
      ],
      "metadata": {
        "id": "X6BsnH_XJW3a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "# rnn_forecast = model_forecast(model, series[...,np.newaxis], N_PAST, BATCH_SIZE)\n",
        "# rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0]\n",
        "# x_valid = x_valid[:rnn_forecast.shape[0]]\n",
        "# result = mae(x_valid, rnn_forecast)\n",
        "# print(\"Result is :\", result) #0.2285"
      ],
      "metadata": {
        "id": "T6ulXBFCJkpS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "# %config InlineBackend.fugure_format = 'retina'\n",
        "# def plot_loss_acc(history):\n",
        "#   #-----------------------------------------------------------\n",
        "#   # Retrieve a list of list results on training and test data\n",
        "#   # sets for each training epoch\n",
        "#   #-----------------------------------------------------------\n",
        "#   mae      = history.history[     'mae' ]\n",
        "#   val_mae  = history.history[ 'val_mae' ]\n",
        "#   loss     = history.history[    'loss' ]\n",
        "#   val_loss = history.history['val_loss' ]\n",
        "#   epochs   = range(len(mae)) # Get number of epochs\n",
        "#   #------------------------------------------------\n",
        "#   # Plot training and validation accuracy per epoch\n",
        "#   #------------------------------------------------\n",
        "#   plt.plot  ( epochs,     mae, label='Training mae' )\n",
        "#   plt.plot  ( epochs, val_mae, label='Validation mae' )\n",
        "#   plt.title ('Training and validation mae')\n",
        "#   plt.grid()\n",
        "#   plt.legend()\n",
        "#   plt.xlabel(\"Epochs\")\n",
        "#   plt.ylabel(\"MAE\")\n",
        "#   plt.figure()\n",
        "#   #------------------------------------------------\n",
        "#   # Plot training and validation loss per epoch\n",
        "#   #------------------------------------------------\n",
        "#   plt.plot  ( epochs,     loss, label='Training loss' )\n",
        "#   plt.plot  ( epochs, val_loss, label='Validation loss' )\n",
        "#   plt.grid()\n",
        "#   plt.legend()\n",
        "#   plt.xlabel(\"Epochs\")\n",
        "#   plt.ylabel(\"Loss\")\n",
        "#   plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "_GSj7wIZ6BnD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot training results\n",
        "# plot_loss_acc(history)"
      ],
      "metadata": {
        "id": "0uOGIaZME1du"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}