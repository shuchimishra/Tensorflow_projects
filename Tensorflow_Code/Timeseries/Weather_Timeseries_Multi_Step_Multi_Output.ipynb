{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/Timeseries/Weather_Timeseries_Multi_Step_Multi_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pa49bUnKyRgF"
      },
      "cell_type": "markdown",
      "source": [
        "# Tensorflow Timeseries LSTM Tutorial\n",
        "\n",
        "In this notebook, I explore the the incredible guide by Tensorflow, [**LINK**](https://www.tensorflow.org/tutorials/structured_data/time_series), and attempt to build off the work by adding Multi-Ouput, Multi-Timestep implimentation.\n",
        "\n",
        "\n",
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/time_series\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "qSAP1TKD8IjV"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial is an introduction to time series forecasting using Recurrent Neural Networks (RNNs). This is covered in two parts: first, you will forecast a univariate time series, then you will forecast a multivariate time series."
      ]
    },
    {
      "metadata": {
        "id": "7rZnJaGTWQw0",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "print(f\"Tensorflow Version: {tf.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "print(f\"Numpy Version: {np.__version__}\")\n",
        "print(f\"System Version: {sys.version}\")\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (17, 5)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "notebookstart= time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Parameters"
      ],
      "metadata": {
        "id": "W90FE4TAYtos"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "n4x4a7BN8IjX"
      },
      "cell_type": "code",
      "source": [
        "# Data Loader Parameters\n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "TRAIN_SPLIT = 300000\n",
        "\n",
        "# LSTM Parameters\n",
        "EVALUATION_INTERVAL = 200\n",
        "EPOCHS = 4\n",
        "PATIENCE = 5\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 13\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TokBlnUhWFw9"
      },
      "cell_type": "markdown",
      "source": [
        "# The weather dataset\n",
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">[weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>.\n",
        "\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by Fran√ßois Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "metadata": {
        "id": "xyv_i85IWInT",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"DataFrame Shape: {} rows, {} columns\".format(*df.shape))\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "WjCHnqDCk1s3"
      }
    },
    {
      "metadata": {
        "id": "qfbpcV0MWQzl"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see above, an observation is recorded every 10 mintues. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations.\n",
        "\n",
        "Given a specific time, let's say you want to predict the temperature 6 hours in the future. In order to make this prediction, you choose to use 5 days of observations. Thus, you would create a window containing the last 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with.\n",
        "\n",
        "The function below returns the above described windows of time for the model to train on. The parameter `history_size` is the size of the past window of information. The `target_size` is how far in the future does the model need to learn to predict. The `target_size` is the label that needs to be predicted."
      ]
    },
    {
      "metadata": {
        "id": "7AoxQuTrWIbi",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i-history_size, i)\n",
        "        # Reshape data from (history_size,) to (history_size, 1)\n",
        "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "        labels.append(dataset[i+target_size])\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoFJZmXBaxCc"
      },
      "cell_type": "markdown",
      "source": [
        "In both the following tutorials, the first 300,000 rows of the data will be the training dataset, and there remaining will be the validation dataset. This amounts to ~2100 days worth of training data."
      ]
    },
    {
      "metadata": {
        "id": "qVukM9dRipop",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def create_time_steps(length):\n",
        "    return list(range(-length, 0))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QQeGvh7cWXMR",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def show_plot(plot_data, delta, title):\n",
        "    labels = ['History', 'True Future', 'Model Prediction']\n",
        "    marker = ['.-', 'rx', 'go']\n",
        "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "    if delta:\n",
        "        future = delta\n",
        "    else:\n",
        "        future = 0\n",
        "\n",
        "    plt.title(title)\n",
        "    for i, x in enumerate(plot_data):\n",
        "        if i:\n",
        "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "                   label=labels[i])\n",
        "        else:\n",
        "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "        plt.legend()\n",
        "        plt.xlim([time_steps[0], (future+5)*2])\n",
        "        plt.xlabel('Time-Step')\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YEwr-NoWUpV"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Forecast a univariate time series\n",
        "First, you will train a model using only a single feature (temperature), and use it to make predictions for that value in the future.\n",
        "\n",
        "Let's first extract only the temperature from the dataset."
      ]
    },
    {
      "metadata": {
        "id": "nbdcnm1_WIY9",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "uni_data = df['T (degC)']\n",
        "uni_data.index = df['Date Time']\n",
        "uni_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQB-46MyWZMm"
      },
      "cell_type": "markdown",
      "source": [
        "Let's observe how this data looks across time."
      ]
    },
    {
      "metadata": {
        "id": "ftOExwAqWXSU",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "uni_data.plot(subplots=True)\n",
        "plt.show()\n",
        "uni_data = uni_data.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eFckdUUHWmT"
      },
      "cell_type": "markdown",
      "source": [
        "It is important to scale features before training a neural network. Standardization is a common way of doing this scaling by subtracting the mean and dividing by the standard deviation of each feature.You could also use a `tf.keras.utils.normalize` method that rescales the values into a range of [0,1]."
      ]
    },
    {
      "metadata": {
        "id": "mxbIic5TMlxx"
      },
      "cell_type": "markdown",
      "source": [
        "Note: The mean and standard deviation should only be computed using the training data."
      ]
    },
    {
      "metadata": {
        "id": "Eji6njXvHusN",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
        "uni_train_std = uni_data[:TRAIN_SPLIT].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Gob1YJYH0cH"
      },
      "cell_type": "markdown",
      "source": [
        "Let's standardize the data."
      ]
    },
    {
      "metadata": {
        "id": "BO55yRD6H0Dx",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "uni_data = (uni_data-uni_train_mean)/uni_train_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gn8A_nrccKtn"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create the data for the univariate model. For part 1, the model will be given the last 20 recorded temperature observations, and needs to learn to predict the temperature at the next time step."
      ]
    },
    {
      "metadata": {
        "id": "aJJ-T49vWXOZ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "univariate_past_history = 20\n",
        "univariate_future_target = 0\n",
        "\n",
        "x_train_uni, y_train_uni = univariate_data(dataset=uni_data,\n",
        "                                           start_index=0,\n",
        "                                           end_index=TRAIN_SPLIT,\n",
        "                                           history_size=univariate_past_history,\n",
        "                                           target_size=univariate_future_target)\n",
        "x_val_uni, y_val_uni = univariate_data(dataset=uni_data,\n",
        "                                       start_index=TRAIN_SPLIT,\n",
        "                                       end_index=None,\n",
        "                                       history_size=univariate_past_history,\n",
        "                                       target_size=univariate_future_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWpVMENsdp0N"
      },
      "cell_type": "markdown",
      "source": [
        "This is what the `univariate_data` function returns."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qRJF_vAT8Ija"
      },
      "cell_type": "code",
      "source": [
        "print(\"In:\")\n",
        "print(uni_data.shape)\n",
        "print(uni_data[:5])\n",
        "\n",
        "print(\"\\nOut\")\n",
        "print(\"Features:\")\n",
        "print(x_train_uni.shape)\n",
        "print(x_val_uni.shape)\n",
        "print(\"\\n\")\n",
        "print(\"Labels:\")\n",
        "print(y_train_uni.shape)\n",
        "print(y_val_uni.shape)\n",
        "\n",
        "print(\"split size is :\", round(x_train_uni.shape[0] / uni_data.shape[0] *100, 2) ,\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "feDd95XFdz5H",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print ('Single window of past history. Shape: {}'.format(x_train_uni[0].shape))\n",
        "print (x_train_uni[0])\n",
        "print ('\\n Target temperature to predict. Shape: {}'.format(y_train_uni[0].shape))\n",
        "print (y_train_uni[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hni3Jt9OMR1_"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the data has been created, let's take a look at a single example. The information given to the network is given in blue, and it must predict the value at the red cross."
      ]
    },
    {
      "metadata": {
        "id": "Pd05iV-UWXKL",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5rUJ_2YMWzG"
      },
      "cell_type": "markdown",
      "source": [
        "### Baseline\n",
        "Before proceeding to train a model, let's first set a simple baseline. Given an input point, the baseline method looks at all the history and predicts the next point to be the average of the last 20 observations."
      ]
    },
    {
      "metadata": {
        "id": "P9nYWcxMMWnr",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def baseline(history):\n",
        "    return np.mean(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMcdFYKQMWlm",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0,\n",
        "           'Baseline Prediction Example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "067m6t8cMakb"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see if you can beat this baseline using a recurrent neural network."
      ]
    },
    {
      "metadata": {
        "id": "H4crpOcoMlSe"
      },
      "cell_type": "markdown",
      "source": [
        "### Recurrent neural network\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state summarizing the information they've seen so far. For more details, read the [RNN tutorial](https://www.tensorflow.org/tutorials/sequences/recurrent). In this tutorial, you will use a specialized RNN layer called Long Short Term Memory ([LSTM](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM))\n",
        "\n",
        "Let's now use `tf.data` to shuffle, batch, and cache the dataset."
      ]
    },
    {
      "metadata": {
        "id": "kk-evkrmMWh9",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
        "# train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_univariate = train_univariate.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
        "# val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2AmKkyVS5Ht"
      },
      "cell_type": "markdown",
      "source": [
        "The following visualisation should help you understand how the data is represented after batching.\n",
        "\n",
        "![Time Series](https://github.com/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/Timeseries/images/time_series.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "4nagdTRNfPuZ"
      },
      "cell_type": "markdown",
      "source": [
        "You will see the LSTM requires the input shape of the data it is being given."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XUR8olNT8Ijc"
      },
      "cell_type": "code",
      "source": [
        "x_train_uni.shape[-2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizer = tf.keras.layers.Normalization()\n",
        "# normalizer.adapt(uni_data)"
      ],
      "metadata": {
        "id": "Y59NnJ9_o1D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDbpHosCMWZO",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# inputs = tf.keras.layers.Input(shape=x_train_uni.shape[-2:])\n",
        "# x = normalizer(inputs)\n",
        "# x = tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:])(x)\n",
        "# outputs = tf.keras.layers.Dense(1)(x)\n",
        "# simple_lstm_model = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOGZtDAqMtSi"
      },
      "cell_type": "markdown",
      "source": [
        "Let's make a sample prediction, to check the output of the model."
      ]
    },
    {
      "metadata": {
        "id": "2mPZbIKCMtLR",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_univariate.take(1):\n",
        "    print(x.shape, y.shape, simple_lstm_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QYz6RN_mMyau"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train the model now. Due to the large size of the dataset, in the interest of saving time, each epoch will only run for 200 steps, instead of the complete training data as normally done."
      ]
    },
    {
      "metadata": {
        "id": "0opH9xi5MtIk",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
        "simple_lstm_model.fit(train_univariate,\n",
        "                      epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate,\n",
        "                      callbacks=[early_stopping],\n",
        "                      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euyPo_lyNryZ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predict using the simple LSTM model\n",
        "Now that you have trained your simple LSTM, let's try and make a few predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(0.604287 * 8.636720398054864 + 9.233256299999999)\n",
        "# print(0.66373146 * 8.636720398054864 + 9.233256299999999)\n",
        "# print(1.3462198 * 8.636720398054864 + 9.233256299999999)"
      ],
      "metadata": {
        "id": "D3vfRQS_n8D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S2rRLrs8MtGU",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_univariate.take(3):\n",
        "    plot = show_plot([x[0].numpy(), y[0].numpy(),\n",
        "                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n",
        "    plot.show()\n",
        "    print(x[0].numpy())\n",
        "    print(y[0].numpy())\n",
        "    print(simple_lstm_model.predict(x)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-AVEJyRNvt0"
      },
      "cell_type": "markdown",
      "source": [
        "This looks better than the baseline. Now that you have seen the basics, let's move on to part two, where you will work with a multivariate time series."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ivxsPZNO8Iji"
      },
      "cell_type": "code",
      "source": [
        "del simple_lstm_model, val_univariate, train_univariate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VlJYi3_HXcw8"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: Forecast a multivariate time series"
      ]
    },
    {
      "metadata": {
        "id": "hoxNZ2GM7DPm"
      },
      "cell_type": "markdown",
      "source": [
        "The original dataset contains fourteen features. For simplicity, this section considers only three of the original fourteen. The features used are air temperature, atmospheric pressure, and air density.\n",
        "\n",
        "To use more features, add their names to this list."
      ]
    },
    {
      "metadata": {
        "id": "DphrB7bxSNDd",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfQUSiJfUpXJ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['Date Time']\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSfhTZi5r15R"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a look at how each of these features vary across time."
      ]
    },
    {
      "metadata": {
        "id": "QdgC8zvGr21X",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cqStgZ-O1b3_"
      },
      "cell_type": "markdown",
      "source": [
        "As mentioned, the first step will be to standardize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "metadata": {
        "id": "W7VuNIwfHRHx",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "dataset = features.values\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
        "dataset = (dataset-data_mean)/data_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KnkNETnJ8Iji"
      },
      "cell_type": "code",
      "source": [
        "display(pd.DataFrame(dataset, columns = features.columns, index= features.index).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyuGuJUgjUK3"
      },
      "cell_type": "markdown",
      "source": [
        "### Single step model\n",
        "In a single step setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as below, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "metadata": {
        "id": "d-rVX4d3OF86",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i-history_size, i, step)\n",
        "        data.append(dataset[indices])\n",
        "\n",
        "        if single_step:\n",
        "            labels.append(target[i+target_size])\n",
        "        else:\n",
        "            labels.append(target[i:i+target_size])\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWVGYwbN2ITI"
      },
      "cell_type": "markdown",
      "source": [
        "In this tutorial, the network is shown data from the last five (5) days, i.e. 720 observations that are sampled every hour. The sampling is done every one hour since a drastic change is not expected within 60 minutes. Thus, 120 observation represent history of the last five days.  For the single step prediction model, the label for a datapoint is the temperature 12 hours into the future. In order to create a label for this, the temperature after 72(12*6) observations is used."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:,1]"
      ],
      "metadata": {
        "id": "fjndP5fTuJn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlhVGzPhmMYI",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0, #dataset[:, 1] indicates temperatures\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CamMObrwPhnp"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at a single data-point.\n"
      ]
    },
    {
      "metadata": {
        "id": "_tVKm-ZIPls0",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print(x_train_single.shape)\n",
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))\n",
        "print(x_train_single.shape[-2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCWG4xgQ3O6E",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
        "# train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_data_single = train_data_single.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
        "# val_data_single = val_data_single.batch(BATCH_SIZE).repeat()\n",
        "val_data_single = val_data_single.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0aWec9_nlxBl",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "single_step_model = tf.keras.models.Sequential()\n",
        "single_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                           input_shape=x_train_single.shape[-2:]))\n",
        "single_step_model.add(tf.keras.layers.Dense(1)) #notice the unit here is 1 since it's single step prediction\n",
        "\n",
        "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYhUfWjwOPFN"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check out a sample prediction."
      ]
    },
    {
      "metadata": {
        "id": "yY7FodHVOPsH",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_single.take(1):\n",
        "    print(single_step_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U0jnt2l2mwkl",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print(f\"Evaluation Threshold: {EVALUATION_INTERVAL}\",\n",
        "      f\"Epochs: {EPOCHS}\", sep=\"\\n\")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
        "single_step_history = single_step_model.fit(train_data_single,\n",
        "                                            epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            validation_data=val_data_single,\n",
        "                                            callbacks=[early_stopping],\n",
        "                                            validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZAdeAnP5c72",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def plot_train_history(history, title):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8lBKA-z5yYV",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(single_step_history,\n",
        "                   'Single Step Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfjrGAlEUp7i"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predict a single step future\n",
        "Now that the model is trained, let's make a few sample predictions. The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction)."
      ]
    },
    {
      "metadata": {
        "id": "h1qmPLLVUpuN",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_single.take(3):\n",
        "    plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
        "                    single_step_model.predict(x)[0]], 12,\n",
        "                   'Single Step Prediction')\n",
        "    plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ktqjiURl8Ijk"
      },
      "cell_type": "code",
      "source": [
        "del single_step_history, val_data_single, train_data_single"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2GnE087bJYSu"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-Step model\n",
        "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predict a sequence of the future.\n",
        "\n",
        "For the multi-step model, the training data again consists of recordings over the past five days sampled every hour. However, here, the model needs to learn to predict the temperature for the next 12 hours. Since an obversation is taken every 10 minutes, the output is 72 predictions. For this task, the dataset needs to be prepared accordingly, thus the first step is just to create it again, but with a different target window."
      ]
    },
    {
      "metadata": {
        "id": "kZCk9fqyJZqX",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LImXPwAGRtWy"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check out a sample data-point."
      ]
    },
    {
      "metadata": {
        "id": "SpWDcBkQRwS-",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print (x_train_multi.shape,\n",
        "       y_train_multi.shape,\n",
        "       'Single window of past history : {}'.format(x_train_multi[0].shape),\n",
        "       'Target temperature to predict : {}'.format(y_train_multi[0].shape),\n",
        "       sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjR4PJArMOpA",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "# train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_data_multi = train_data_multi.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "# val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZcg8FWpSG8K"
      },
      "cell_type": "markdown",
      "source": [
        "Plotting a sample data-point."
      ]
    },
    {
      "metadata": {
        "id": "ksXKVbwBV7D3",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    num_in = create_time_steps(len(history))\n",
        "    num_out = len(true_future)\n",
        "\n",
        "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "    if prediction.any():\n",
        "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "                 label='Predicted Future')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCQKetflZRMF"
      },
      "cell_type": "markdown",
      "source": [
        "In this plot and subsequent similar plots, the history and the future data are sampled every hour."
      ]
    },
    {
      "metadata": {
        "id": "R6G8bacQR4w2",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in train_data_multi.take(1):\n",
        "    multi_step_plot(x[0], y[0], np.array([0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XOjz8DzZ4HFS"
      },
      "cell_type": "markdown",
      "source": [
        "Since the task here is a bit more complicated than the previous task, the model now consists of two LSTM layers. Finally, since 72 predictions are made, the dense layer outputs 72 predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_multi.shape[-2:]"
      ],
      "metadata": {
        "id": "x_RBonBf-Acw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "byAl0NKSNBP6",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=x_train_multi.shape[-2:]))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.Dense(72)) #V. IMP notice the unit here is 72 since it's multi step prediction\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
        "print(multi_step_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvB7zBqVSMyl"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see how the model predicts before it trains."
      ]
    },
    {
      "metadata": {
        "id": "13_ZWvB9SRlZ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_multi.take(1):\n",
        "    print (multi_step_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uwOhXo3Oems",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n",
        "multi_step_history = multi_step_model.fit(train_data_multi,\n",
        "                                          epochs=EPOCHS,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=EVALUATION_INTERVAL,\n",
        "                                          callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKfQoBjQ5l7U",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDg94-yq4pas"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predict a multi-step future\n",
        "Let's now have a look at how well your network has learnt to predict the future."
      ]
    },
    {
      "metadata": {
        "id": "dt22wq6fyIBU",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_multi.take(3):\n",
        "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "g-gX43P38Ijl"
      },
      "cell_type": "code",
      "source": [
        "del multi_step_model, val_data_multi, train_data_multi\n",
        "_ = gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYVkWFWO8Ijl"
      },
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "This tutorial was a quick introduction to time series forecasting using an RNN. You may now try to predict the stock market and become a billionaire.\n",
        "\n",
        "In addition, you may also write a generator to yield data (instead of the uni/multivariate_data function), which would be more memory efficient. You may also check out this [time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing) guide and use it in this tutorial.\n",
        "\n",
        "For further understanding, you may read Chapter 15 of [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), 2nd Edition and Chapter 6 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "metadata": {
        "id": "yNdw8PZ18Ijl"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-Step, Multi-Input, and Multi-Output\n",
        "\n",
        "_By Nick Brooks, Feb 2020_\n",
        "\n",
        "Inspired by the following paper:\n",
        "\n",
        "- https://arxiv.org/abs/1903.02791\n",
        "- https://github.com/niklascp/bus-arrival-convlstm/tree/master/jupyter"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lIj_Zuu98Ijm"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QyG_DgcI8Ijm"
      },
      "cell_type": "code",
      "source": [
        "def multivariate_multioutput_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i-history_size, i, step)\n",
        "        data.append(dataset[indices])\n",
        "\n",
        "        if single_step:\n",
        "            labels.append(target[i+target_size])\n",
        "        else:\n",
        "            labels.append(target[i:i+target_size])\n",
        "\n",
        "    return np.array(data)[:,:,:,np.newaxis,np.newaxis], np.array(labels)[:,:,:,np.newaxis,np.newaxis]\n",
        "\n",
        "def multi_step_output_plot(history, true_future, prediction):\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    num_in = create_time_steps(len(history))\n",
        "    num_out = len(true_future)\n",
        "\n",
        "    for i, (var, c) in enumerate(zip(features.columns[:2], ['b','r'])):\n",
        "        plt.plot(num_in, np.array(history[:, i]), c, label=var)\n",
        "        plt.plot(np.arange(num_out)/STEP, np.array(true_future[:,i]), c+'o', markersize=5, alpha=0.5,\n",
        "               label=f\"True {var.title()}\")\n",
        "        if prediction.any():\n",
        "            plt.plot(np.arange(num_out)/STEP, np.array(prediction[:,i]), '*', markersize=5, alpha=0.5,\n",
        "                     label=f\"Predicted {var.title()}\")\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features\n"
      ],
      "metadata": {
        "id": "-LmWKMAFMYOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we predict two features - p (mbar) &\tT (degC)"
      ],
      "metadata": {
        "id": "1IVcE7CSPxnF"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UU8eLEpw8Ijm"
      },
      "cell_type": "code",
      "source": [
        "future_target = 72\n",
        "x_train_multi, y_train_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:,:2], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:, :2],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ME_nvAk18Ijm"
      },
      "cell_type": "code",
      "source": [
        "print (x_train_multi.shape,\n",
        "       y_train_multi.shape,\n",
        "       x_val_multi.shape,\n",
        "       y_val_multi.shape,\n",
        "       'Single window of past history : {}'.format(x_train_multi[0].shape),\n",
        "       'Target temperature to predict : {}'.format(y_train_multi[0].shape),\n",
        "       sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "89nW2yjw8Ijm"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zc8LKIEhPnW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q0z_owtt8Ijm"
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_multi.take(10):\n",
        "    multi_step_output_plot(np.squeeze(x[0]), np.squeeze(y[0]), np.array([0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjAVBjR18Ijm"
      },
      "cell_type": "markdown",
      "source": [
        "### Convolutional LSTM\n",
        "\n",
        "As taken from the paper."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nb_cQ5cH8Ijm"
      },
      "cell_type": "code",
      "source": [
        "def build_model(input_timesteps, output_timesteps, num_links, num_inputs):\n",
        "    # COPY PASTA\n",
        "    # https://github.com/niklascp/bus-arrival-convlstm/blob/master/jupyter/ConvLSTM_3x15min_10x64-5x64-10x64-5x64-Comparison.ipynb\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(name = 'batch_norm_0', input_shape = (input_timesteps, num_inputs, 1, 1)))\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_1',\n",
        "                         filters = 64, kernel_size = (10, 1),\n",
        "                         padding = 'same',\n",
        "                         return_sequences = True))\n",
        "\n",
        "    model.add(Dropout(0.30, name = 'dropout_1'))\n",
        "    model.add(BatchNormalization(name = 'batch_norm_1'))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_2',\n",
        "                         filters = 64, kernel_size = (5, 1),\n",
        "                         padding='same',\n",
        "                         return_sequences = False))\n",
        "\n",
        "    model.add(Dropout(0.20, name = 'dropout_2'))\n",
        "    model.add(BatchNormalization(name = 'batch_norm_2'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(RepeatVector(output_timesteps))\n",
        "    model.add(Reshape((output_timesteps, num_inputs, 1, 64)))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_3',\n",
        "                         filters = 64, kernel_size = (10, 1),\n",
        "                         padding='same',\n",
        "                         return_sequences = True))\n",
        "\n",
        "    model.add(Dropout(0.20, name = 'dropout_3'))\n",
        "    model.add(BatchNormalization(name = 'batch_norm_3'))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_4',\n",
        "                         filters = 64, kernel_size = (5, 1),\n",
        "                         padding='same',\n",
        "                         return_sequences = True))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(units=1, name = 'dense_1', activation = 'relu')))\n",
        "    model.add(Dense(units=1, name = 'dense_2', activation = 'linear'))\n",
        "\n",
        "#     optimizer = RMSprop() #lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.9)\n",
        "#     optimizer = tf.keras.optimizers.Adam(0.1)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=0.004, clipvalue=1.0)\n",
        "    model.compile(loss = \"mse\", optimizer = optimizer, metrics = ['mae', 'mse'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LgYpUmdX8Ijm"
      },
      "cell_type": "code",
      "source": [
        "future_target = 72\n",
        "x_train_multi, y_train_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:,:2], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:, :2],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "afJe193a8Ijm"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 40\n",
        "steps_per_epoch = 350\n",
        "validation_steps = 500\n",
        "\n",
        "modelstart = time.time()\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience = PATIENCE, restore_best_weights=True)\n",
        "model = build_model(x_train_multi.shape[1], future_target, y_train_multi.shape[2], x_train_multi.shape[2])\n",
        "print(model.summary())\n",
        "\n",
        "# Train\n",
        "print(\"\\nTRAIN MODEL...\")\n",
        "history = model.fit(train_data_multi,\n",
        "                    epochs = EPOCHS,\n",
        "                    validation_data=val_data_multi,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "model.save('multi-output-timesteps.h5')\n",
        "print(\"\\nModel Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hor_XHtE8Ijm"
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(history, 'Multi-Step, Multi-Output Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "SqwLgV1n8Ijn"
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_multi.take(10):\n",
        "    multi_step_output_plot(np.squeeze(x[0]), np.squeeze(y[0]), np.squeeze(model.predict(x[0][np.newaxis,:,:,:,:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCvcgw7C8Ijn"
      },
      "cell_type": "markdown",
      "source": [
        "### Simplified the Convolutional LSTM"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jkYKAaKA8Ijn"
      },
      "cell_type": "code",
      "source": [
        "def build_model(input_timesteps, output_timesteps, num_links, num_inputs):\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(name = 'batch_norm_0', input_shape = (input_timesteps, num_inputs, 1, 1)))\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_1',\n",
        "                         filters = 64, kernel_size = (10, 1),\n",
        "                         padding = 'same',\n",
        "                         return_sequences = False))\n",
        "\n",
        "    model.add(Dropout(0.30, name = 'dropout_1'))\n",
        "    model.add(BatchNormalization(name = 'batch_norm_1'))\n",
        "\n",
        "#     model.add(ConvLSTM2D(name ='conv_lstm_2',\n",
        "#                          filters = 64, kernel_size = (5, 1),\n",
        "#                          padding='same',\n",
        "#                          return_sequences = False))\n",
        "\n",
        "#     model.add(Dropout(0.20, name = 'dropout_2'))\n",
        "#     model.add(BatchNormalization(name = 'batch_norm_2'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(RepeatVector(output_timesteps))\n",
        "    model.add(Reshape((output_timesteps, num_inputs, 1, 64)))\n",
        "\n",
        "#     model.add(ConvLSTM2D(name ='conv_lstm_3',\n",
        "#                          filters = 64, kernel_size = (10, 1),\n",
        "#                          padding='same',\n",
        "#                          return_sequences = True))\n",
        "\n",
        "#     model.add(Dropout(0.20, name = 'dropout_3'))\n",
        "#     model.add(BatchNormalization(name = 'batch_norm_3'))\n",
        "\n",
        "    model.add(ConvLSTM2D(name ='conv_lstm_4',\n",
        "                         filters = 64, kernel_size = (5, 1),\n",
        "                         padding='same',\n",
        "                         return_sequences = True))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(units=1, name = 'dense_1', activation = 'relu')))\n",
        "    model.add(Dense(units=1, name = 'dense_2'))\n",
        "\n",
        "#     optimizer = RMSprop() #lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.9)\n",
        "#     optimizer = tf.keras.optimizers.Adam(0.1)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=0.003, clipvalue=1.0)\n",
        "    model.compile(loss = \"mse\", optimizer = optimizer, metrics = ['mae', 'mse'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rAsanz6t8Ijn"
      },
      "cell_type": "code",
      "source": [
        "# Extend Prediction Window..\n",
        "future_target = 144\n",
        "x_train_multi, y_train_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:,:2], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_multioutput_data(dataset[:,:2], dataset[:, :2],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ORrXt_wj8Ijn"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "steps_per_epoch = 350\n",
        "validation_steps = 500\n",
        "\n",
        "modelstart = time.time()\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience = PATIENCE, restore_best_weights=True)\n",
        "model = build_model(x_train_multi.shape[1], future_target, y_train_multi.shape[2], x_train_multi.shape[2])\n",
        "print(model.summary())\n",
        "\n",
        "# Train\n",
        "print(\"\\nTRAIN MODEL...\")\n",
        "history = model.fit(train_data_multi,\n",
        "                    epochs = EPOCHS,\n",
        "                    validation_data=val_data_multi,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "model.save('multi-output-timesteps.h5')\n",
        "print(\"\\nModel Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QHUBqkNE8Ijn"
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(history, 'Multi-Step, Multi-Output Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "h8votq_58Ijn"
      },
      "cell_type": "code",
      "source": [
        "for x, y in val_data_multi.take(10):\n",
        "    multi_step_output_plot(np.squeeze(x[0]), np.squeeze(y[0]), np.squeeze(model.predict(x[0][np.newaxis,:,:,:,:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CKuneoTL8Ijn"
      },
      "cell_type": "code",
      "source": [
        "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ScdfSBmW8Ijn"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "name": "Keras Timeseries Multi-Step Multi-Output",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}