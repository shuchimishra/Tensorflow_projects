{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOl4iaXGKSzfDU24bnYI3Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/Intro_to_ANN/exam/Category2_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAvApmbyuzRi"
      },
      "outputs": [],
      "source": [
        "# # Question\n",
        "# #\n",
        "# # Create a classifier for the CIFAR10 dataset\n",
        "# # Note that the test will expect it to classify 10 classes and that the input shape should be\n",
        "# # the native CIFAR size which is 32x32 pixels with 3 bytes color depth\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import *\n",
        "# def solution_model():\n",
        "#     # cifar = tf.keras.datasets.cifar10\n",
        "#     (x_train,y_train), (x_test,y_test) =  tf.keras.datasets.cifar10.load_data()\n",
        "#     #2.모델구성\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(16,(3,3),input_shape = (32,32,3),activation='swish'))\n",
        "#     model.add(Conv2D(24,(3,3),activation='swish'))\n",
        "#     model.add(Conv2D(32,(3,3),activation='swish'))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "#     #3.컴파일 훈련\n",
        "#     model.compile(loss='SparseCategorical_Crossentropy',optimizer='adam',metrics='accuracy')\n",
        "#     model.fit(x_train,y_train,epochs=100,verbose=1,validation_split=0.1)\n",
        "\n",
        "#     loss = model.evaluate(x_test,y_test)\n",
        "#     print('loss:',loss[0])\n",
        "#     print('acc:',loss[1])\n",
        "\n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "# # Note that you'll need to save your model as a .h5 like this\n",
        "# # This .h5 will be uploaded to the testing infrastructure\n",
        "# # and a score will be returned to you\n",
        "# if __name__ == '__main__':\n",
        "#     model = solution_model()\n",
        "#     model.save(\"mymodel.h5\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense\n",
        "\n",
        "def solution_model():\n",
        "    # 데이터 불러오기\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "    # 모델 구성\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
        "    model.add(Conv2D(24, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # 컴파일, 훈련\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=10, verbose=1, validation_split=0.1)\n",
        "\n",
        "    # 모델 평가\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    print('loss:', loss)\n",
        "    print('accuracy:', accuracy)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 저장\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    # model.save(\"mymodel.h5\")\n",
        "'''\n",
        "loss: 3.0625479221343994\n",
        "accuracy: 0.49950000643730164\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question\n",
        "#\n",
        "# Create a classifier for the CIFAR10 dataset\n",
        "# Note that the test will expect it to classify 10 classes and that the input shape should be\n",
        "# the native CIFAR size which is 32x32 pixels with 3 bytes color depth\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense\n",
        "from tensorflow import keras\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class myCallBack(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if (logs['val_accuracy'] > 0.99):\n",
        "      print(\"Cancelling training since accuracy reached 99%\")\n",
        "      self.model.stop_training=True\n",
        "\n",
        "def solution_model():\n",
        "\n",
        "  (train_images,train_labels),(test_images,test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "  #number of classes\n",
        "  n_classes = len(set(train_labels[:,0]))\n",
        "\n",
        "  #normalize image\n",
        "  train_images = train_images/255.0\n",
        "  test_images = test_images/255.0\n",
        "\n",
        "  #callbacks\n",
        "  callback = myCallBack()\n",
        "  MCP = keras.callbacks.ModelCheckpoint(filepath='bestmodel.h5',monitor='val_accuracy',\n",
        "                                        mode='auto',save_best_only=True,save_weights_only=False,verbose=1)\n",
        "  RLP = keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",factor=0.1,patience=3,\n",
        "                                          verbose=1,mode=\"auto\",min_lr=0.000000001)\n",
        "  ES = keras.callbacks.EarlyStopping(monitor='val_accuracy',min_delta=1e-4,patience=5,verbose=1)\n",
        "\n",
        "  model = keras.models.Sequential([\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(32, 32, 3)), #filter=64\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D((2,2)),\n",
        "      # keras.layers.Dropout(0.5),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dropout(0.2),\n",
        "      # keras.layers.Dense(1024, activation='relu'), #512\n",
        "      # keras.layers.Dense(512, activation='relu'), #512\n",
        "      keras.layers.Dense(128, activation='relu'), #512\n",
        "      keras.layers.Dense(n_classes, activation='softmax')])\n",
        "\n",
        "  lr = 0.0001\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics='accuracy')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  history = model.fit(\n",
        "        train_images, train_labels,\n",
        "        # batch_size=512,\n",
        "        epochs=50,\n",
        "        verbose=1,\n",
        "        validation_data=(test_images, test_labels),\n",
        "        callbacks=[callback,MCP,RLP,ES] #RLP,ES\n",
        "    )\n",
        "\n",
        "  # model.summary()\n",
        "  #Evaluate against test images\n",
        "  model.evaluate(test_images, test_labels)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wQIU7gqVB8Y0",
        "outputId": "15a882d4-b31a-4fdb-e50b-458470db98f6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 128)       3584      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 365706 (1.40 MB)\n",
            "Trainable params: 365706 (1.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 1.7764 - accuracy: 0.3438\n",
            "Epoch 1: val_accuracy improved from -inf to 0.47080, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.7753 - accuracy: 0.3441 - val_loss: 1.4806 - val_accuracy: 0.4708 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            " 518/1563 [========>.....................] - ETA: 3s - loss: 1.4991 - accuracy: 0.4575"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-30d667a6f2fe>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# and a score will be returned to you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mymodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-30d667a6f2fe>\u001b[0m in \u001b[0;36msolution_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   history = model.fit(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# batch_size=512,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    632\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \"\"\"\n\u001b[1;32m   1065\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Conv2D, Flatten, Dense\n",
        "# from tensorflow import keras\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "WRRk-F0-vA4D"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class myCallBack(keras.callbacks.Callback):\n",
        "#   def on_epoch_end(self, epoch, logs={}):\n",
        "#     if (logs['val_accuracy'] > 0.99):\n",
        "#       print(\"Cancelling training since accuracy reached 99%\")\n",
        "#       self.model.stop_training=True"
      ],
      "metadata": {
        "id": "72dc6ti4ys62"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (train_images,train_labels),(test_images,test_labels) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEmcDMysvFPw",
        "outputId": "701e2263-ef5e-424a-d9d9-6e64d39096de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 17s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #number of classes\n",
        "# n_classes = len(set(train_labels[:,0]))"
      ],
      "metadata": {
        "id": "u5jiPjyfwu2U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #normalize image\n",
        "# train_images = train_images/255.0\n",
        "# test_images = test_images/255.0"
      ],
      "metadata": {
        "id": "b_jNafQ_w32H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn3O7oUa5el-",
        "outputId": "e06f36c1-60f3-46da-c257-beb22614c89b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #callbacks\n",
        "# callback = myCallBack()\n",
        "# MCP = keras.callbacks.ModelCheckpoint(filepath='bestmodel.h5',monitor='val_accuracy',\n",
        "#                                       mode='auto',save_best_only=True,save_weights_only=False,verbose=1)\n",
        "# RLP = keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",factor=0.1,patience=3,\n",
        "#                                         verbose=1,mode=\"auto\",min_lr=0.000000001)\n",
        "# ES = keras.callbacks.EarlyStopping(monitor='val_accuracy',min_delta=1e-4,patience=5,verbose=1)"
      ],
      "metadata": {
        "id": "tEJyfRN6w5u7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.Sequential([\n",
        "#     keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(32, 32, 3)), #filter=64\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     # keras.layers.Dropout(0.5),\n",
        "#     keras.layers.Flatten(),\n",
        "#     keras.layers.Dropout(0.2),\n",
        "#     # keras.layers.Dense(1024, activation='relu'), #512\n",
        "#     # keras.layers.Dense(512, activation='relu'), #512\n",
        "#     keras.layers.Dense(128, activation='relu'), #512\n",
        "#     keras.layers.Dense(n_classes, activation='softmax')])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W_sVJEhxFgZ",
        "outputId": "04ab3f1a-99a5-4968-ede5-40d103740e34"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 128)       3584      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 365706 (1.40 MB)\n",
            "Trainable params: 365706 (1.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = 0.0001\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# model.compile(optimizer=optimizer,\n",
        "#                 loss='sparse_categorical_crossentropy',\n",
        "#                 metrics='accuracy')"
      ],
      "metadata": {
        "id": "jaeBfxg6xTXt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "# history = model.fit(\n",
        "#       train_images, train_labels,\n",
        "#       # batch_size=512,\n",
        "#       epochs=50,\n",
        "#       verbose=1,\n",
        "#       validation_data=(test_images, test_labels),\n",
        "#       callbacks=[callback,MCP,RLP,ES] #RLP,ES\n",
        "#   )\n",
        "\n",
        "# # model.summary()\n",
        "# #Evaluate against test images\n",
        "# model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnnk0cVuxZqE",
        "outputId": "c178a035-83ba-4132-d9ef-d30b7c8a93f8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.7712 - accuracy: 0.3493\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46660, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 16s 9ms/step - loss: 1.7712 - accuracy: 0.3493 - val_loss: 1.4851 - val_accuracy: 0.4666 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 1.4349 - accuracy: 0.4819\n",
            "Epoch 2: val_accuracy improved from 0.46660 to 0.52320, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4347 - accuracy: 0.4820 - val_loss: 1.3172 - val_accuracy: 0.5232 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 1.3134 - accuracy: 0.5314\n",
            "Epoch 3: val_accuracy improved from 0.52320 to 0.56380, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3129 - accuracy: 0.5316 - val_loss: 1.2223 - val_accuracy: 0.5638 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "1551/1563 [============================>.] - ETA: 0s - loss: 1.2302 - accuracy: 0.5637\n",
            "Epoch 4: val_accuracy improved from 0.56380 to 0.58510, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2302 - accuracy: 0.5637 - val_loss: 1.1782 - val_accuracy: 0.5851 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 1.1550 - accuracy: 0.5933\n",
            "Epoch 5: val_accuracy improved from 0.58510 to 0.60260, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1560 - accuracy: 0.5927 - val_loss: 1.1335 - val_accuracy: 0.6026 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 1.0961 - accuracy: 0.6167\n",
            "Epoch 6: val_accuracy improved from 0.60260 to 0.63450, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0961 - accuracy: 0.6167 - val_loss: 1.0514 - val_accuracy: 0.6345 - lr: 1.0000e-04\n",
            "Epoch 7/50\n",
            "1554/1563 [============================>.] - ETA: 0s - loss: 1.0447 - accuracy: 0.6321\n",
            "Epoch 7: val_accuracy improved from 0.63450 to 0.64720, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0451 - accuracy: 0.6319 - val_loss: 1.0054 - val_accuracy: 0.6472 - lr: 1.0000e-04\n",
            "Epoch 8/50\n",
            "1552/1563 [============================>.] - ETA: 0s - loss: 1.0004 - accuracy: 0.6517\n",
            "Epoch 8: val_accuracy improved from 0.64720 to 0.66090, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0010 - accuracy: 0.6514 - val_loss: 0.9795 - val_accuracy: 0.6609 - lr: 1.0000e-04\n",
            "Epoch 9/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.9601 - accuracy: 0.6648\n",
            "Epoch 9: val_accuracy did not improve from 0.66090\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.9602 - accuracy: 0.6648 - val_loss: 1.0044 - val_accuracy: 0.6506 - lr: 1.0000e-04\n",
            "Epoch 10/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.9295 - accuracy: 0.6760\n",
            "Epoch 10: val_accuracy improved from 0.66090 to 0.68310, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.9295 - accuracy: 0.6760 - val_loss: 0.9145 - val_accuracy: 0.6831 - lr: 1.0000e-04\n",
            "Epoch 11/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 0.8946 - accuracy: 0.6883\n",
            "Epoch 11: val_accuracy improved from 0.68310 to 0.68630, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8953 - accuracy: 0.6881 - val_loss: 0.9036 - val_accuracy: 0.6863 - lr: 1.0000e-04\n",
            "Epoch 12/50\n",
            "1551/1563 [============================>.] - ETA: 0s - loss: 0.8648 - accuracy: 0.6976\n",
            "Epoch 12: val_accuracy improved from 0.68630 to 0.69390, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8645 - accuracy: 0.6977 - val_loss: 0.9011 - val_accuracy: 0.6939 - lr: 1.0000e-04\n",
            "Epoch 13/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8370 - accuracy: 0.7080\n",
            "Epoch 13: val_accuracy improved from 0.69390 to 0.71410, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8369 - accuracy: 0.7080 - val_loss: 0.8422 - val_accuracy: 0.7141 - lr: 1.0000e-04\n",
            "Epoch 14/50\n",
            "1555/1563 [============================>.] - ETA: 0s - loss: 0.8127 - accuracy: 0.7190\n",
            "Epoch 14: val_accuracy did not improve from 0.71410\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8124 - accuracy: 0.7191 - val_loss: 0.8372 - val_accuracy: 0.7091 - lr: 1.0000e-04\n",
            "Epoch 15/50\n",
            "1554/1563 [============================>.] - ETA: 0s - loss: 0.7880 - accuracy: 0.7255\n",
            "Epoch 15: val_accuracy improved from 0.71410 to 0.71930, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7886 - accuracy: 0.7254 - val_loss: 0.8145 - val_accuracy: 0.7193 - lr: 1.0000e-04\n",
            "Epoch 16/50\n",
            "1552/1563 [============================>.] - ETA: 0s - loss: 0.7667 - accuracy: 0.7349\n",
            "Epoch 16: val_accuracy improved from 0.71930 to 0.72040, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7670 - accuracy: 0.7346 - val_loss: 0.8148 - val_accuracy: 0.7204 - lr: 1.0000e-04\n",
            "Epoch 17/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.7416 - accuracy: 0.7417\n",
            "Epoch 17: val_accuracy improved from 0.72040 to 0.72990, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7418 - accuracy: 0.7417 - val_loss: 0.7962 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.7482\n",
            "Epoch 18: val_accuracy did not improve from 0.72990\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.7259 - accuracy: 0.7482 - val_loss: 0.7999 - val_accuracy: 0.7283 - lr: 1.0000e-04\n",
            "Epoch 19/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.7082 - accuracy: 0.7544\n",
            "Epoch 19: val_accuracy improved from 0.72990 to 0.73220, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7081 - accuracy: 0.7544 - val_loss: 0.7890 - val_accuracy: 0.7322 - lr: 1.0000e-04\n",
            "Epoch 20/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.6899 - accuracy: 0.7607\n",
            "Epoch 20: val_accuracy did not improve from 0.73220\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6899 - accuracy: 0.7608 - val_loss: 0.7896 - val_accuracy: 0.7304 - lr: 1.0000e-04\n",
            "Epoch 21/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.6738 - accuracy: 0.7670\n",
            "Epoch 21: val_accuracy improved from 0.73220 to 0.74150, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6738 - accuracy: 0.7670 - val_loss: 0.7617 - val_accuracy: 0.7415 - lr: 1.0000e-04\n",
            "Epoch 22/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.6559 - accuracy: 0.7727\n",
            "Epoch 22: val_accuracy did not improve from 0.74150\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6558 - accuracy: 0.7727 - val_loss: 0.7734 - val_accuracy: 0.7371 - lr: 1.0000e-04\n",
            "Epoch 23/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6384 - accuracy: 0.7784\n",
            "Epoch 23: val_accuracy improved from 0.74150 to 0.74850, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.6385 - accuracy: 0.7784 - val_loss: 0.7482 - val_accuracy: 0.7485 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.6178 - accuracy: 0.7859\n",
            "Epoch 24: val_accuracy improved from 0.74850 to 0.74920, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.6178 - accuracy: 0.7859 - val_loss: 0.7477 - val_accuracy: 0.7492 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 0.6048 - accuracy: 0.7898\n",
            "Epoch 25: val_accuracy improved from 0.74920 to 0.74950, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.6049 - accuracy: 0.7897 - val_loss: 0.7381 - val_accuracy: 0.7495 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.7936\n",
            "Epoch 26: val_accuracy did not improve from 0.74950\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5928 - accuracy: 0.7937 - val_loss: 0.7753 - val_accuracy: 0.7418 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7993\n",
            "Epoch 27: val_accuracy improved from 0.74950 to 0.75580, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.5749 - accuracy: 0.7994 - val_loss: 0.7227 - val_accuracy: 0.7558 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.8045\n",
            "Epoch 28: val_accuracy did not improve from 0.75580\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.5624 - accuracy: 0.8045 - val_loss: 0.7311 - val_accuracy: 0.7550 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.8085\n",
            "Epoch 29: val_accuracy improved from 0.75580 to 0.76250, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5487 - accuracy: 0.8084 - val_loss: 0.7147 - val_accuracy: 0.7625 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 0.5361 - accuracy: 0.8134\n",
            "Epoch 30: val_accuracy did not improve from 0.76250\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5361 - accuracy: 0.8134 - val_loss: 0.7155 - val_accuracy: 0.7581 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 0.5187 - accuracy: 0.8193\n",
            "Epoch 31: val_accuracy improved from 0.76250 to 0.76420, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5186 - accuracy: 0.8193 - val_loss: 0.7156 - val_accuracy: 0.7642 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.5099 - accuracy: 0.8233\n",
            "Epoch 32: val_accuracy did not improve from 0.76420\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5099 - accuracy: 0.8233 - val_loss: 0.7349 - val_accuracy: 0.7578 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.4949 - accuracy: 0.8274\n",
            "Epoch 33: val_accuracy did not improve from 0.76420\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4950 - accuracy: 0.8275 - val_loss: 0.7268 - val_accuracy: 0.7565 - lr: 1.0000e-04\n",
            "Epoch 34/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8295\n",
            "Epoch 34: val_accuracy did not improve from 0.76420\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4851 - accuracy: 0.8295 - val_loss: 0.7540 - val_accuracy: 0.7524 - lr: 1.0000e-04\n",
            "Epoch 35/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8565\n",
            "Epoch 35: val_accuracy improved from 0.76420 to 0.77010, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4233 - accuracy: 0.8565 - val_loss: 0.6969 - val_accuracy: 0.7701 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 0.4151 - accuracy: 0.8571\n",
            "Epoch 36: val_accuracy improved from 0.77010 to 0.77090, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.4154 - accuracy: 0.8569 - val_loss: 0.7012 - val_accuracy: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "1554/1563 [============================>.] - ETA: 0s - loss: 0.4123 - accuracy: 0.8581\n",
            "Epoch 37: val_accuracy did not improve from 0.77090\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.4118 - accuracy: 0.8582 - val_loss: 0.6986 - val_accuracy: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8603\n",
            "Epoch 38: val_accuracy improved from 0.77090 to 0.77120, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4076 - accuracy: 0.8603 - val_loss: 0.6979 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "1552/1563 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8607\n",
            "Epoch 39: val_accuracy did not improve from 0.77120\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4056 - accuracy: 0.8608 - val_loss: 0.7015 - val_accuracy: 0.7693 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8601\n",
            "Epoch 40: val_accuracy did not improve from 0.77120\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.4051 - accuracy: 0.8601 - val_loss: 0.6991 - val_accuracy: 0.7711 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.4039 - accuracy: 0.8603\n",
            "Epoch 41: val_accuracy did not improve from 0.77120\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4039 - accuracy: 0.8603 - val_loss: 0.7023 - val_accuracy: 0.7685 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8635\n",
            "Epoch 42: val_accuracy improved from 0.77120 to 0.77140, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3962 - accuracy: 0.8635 - val_loss: 0.6964 - val_accuracy: 0.7714 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8647\n",
            "Epoch 43: val_accuracy improved from 0.77140 to 0.77290, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3944 - accuracy: 0.8647 - val_loss: 0.6967 - val_accuracy: 0.7729 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8650\n",
            "Epoch 44: val_accuracy improved from 0.77290 to 0.77300, saving model to bestmodel.h5\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3944 - accuracy: 0.8650 - val_loss: 0.6965 - val_accuracy: 0.7730 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8650\n",
            "Epoch 45: val_accuracy did not improve from 0.77300\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3952 - accuracy: 0.8651 - val_loss: 0.6968 - val_accuracy: 0.7728 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.3931 - accuracy: 0.8644\n",
            "Epoch 46: val_accuracy did not improve from 0.77300\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3930 - accuracy: 0.8645 - val_loss: 0.6971 - val_accuracy: 0.7727 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8649\n",
            "Epoch 47: val_accuracy did not improve from 0.77300\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3959 - accuracy: 0.8651 - val_loss: 0.6967 - val_accuracy: 0.7713 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.3930 - accuracy: 0.8659\n",
            "Epoch 48: val_accuracy did not improve from 0.77300\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3932 - accuracy: 0.8658 - val_loss: 0.6965 - val_accuracy: 0.7715 - lr: 1.0000e-07\n",
            "Epoch 49/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8629\n",
            "Epoch 49: val_accuracy did not improve from 0.77300\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3943 - accuracy: 0.8629 - val_loss: 0.6964 - val_accuracy: 0.7719 - lr: 1.0000e-07\n",
            "Epoch 49: early stopping\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6964 - accuracy: 0.7719\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6964036226272583, 0.7718999981880188]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "id": "5ZJt9au-Bst5"
      },
      "execution_count": 63,
      "outputs": []
    }
  ]
}