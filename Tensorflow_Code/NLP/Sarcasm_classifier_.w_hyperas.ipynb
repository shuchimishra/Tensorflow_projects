{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/NLP/Sarcasm_classifier_.w_hyperas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfVtdwryvvP6"
      },
      "source": [
        "# Ungraded Lab: Training a binary classifier with the Sarcasm Dataset\n",
        "\n",
        "In this lab, you will revisit the [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home) from last week and proceed to build a train a model on it. The steps will be very similar to the previous lab with IMDB Reviews with just some minor modifications. You can tweak the hyperparameters and see how it affects the results. Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NkeugWZrdTh"
      },
      "source": [
        "Reference article - https://medium.com/@fiona.s.feng/hyperparameter-tuning-for-text-classification-in-keras-tensorflow-with-hyperas-86668a7e732b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA0KccfDranJ"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall hyperas -Y\n",
        "# !pip uninstall hyperopt -Y\n",
        "!pip install git+https://github.com/maxpumperla/hyperas.git#egg=hyperas\n",
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmzwCGbaLok"
      },
      "source": [
        "Ensure you have right version with this line -\n",
        "**rstate=np.random.default_rng(rseed)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ0ewqZMXdno"
      },
      "outputs": [],
      "source": [
        "!nl -b a  /usr/local/lib/python3.10/dist-packages/hyperas/optim.py | grep 142"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQUsBLQQ73W9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive'\n",
        "            # ,force_remount=True\n",
        "            )\n",
        "%ls /gdrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIM6gplHqfx"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "You will first download the JSON file, load it into your workspace and put the sentences and labels into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQVuQrZNkPn9"
      },
      "outputs": [],
      "source": [
        "# # Download the dataset\n",
        "# !wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qciTzNR7IHzJ"
      },
      "source": [
        "## Preprocessing the train and test sets\n",
        "\n",
        "Now you can preprocess the text and labels so it can be consumed by the model. You use the `Tokenizer` class to create the vocabulary and the `pad_sequences` method to generate padded token sequences. You will also need to set the labels to a numpy array so it can be a valid data type for `model.fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV7pO9-PUeaJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "\n",
        "def data():\n",
        "\n",
        "  filename = './sarcasm.json'\n",
        "\n",
        "  if os.path.isfile('./sarcasm.json'):\n",
        "    print(\"skipping the download\")\n",
        "  else:\n",
        "    # Download the dataset\n",
        "    !wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
        "\n",
        "\n",
        "  ## Opening JSON file\n",
        "  f = open(filename)\n",
        "\n",
        "  # Load the json file\n",
        "  datastore = json.load(f)\n",
        "\n",
        "  # Initializing the lists\n",
        "  urls, texts, labels = [],[],[]\n",
        "\n",
        "  #Iterating through json data\n",
        "  for row in datastore:\n",
        "    urls.append(row['article_link'])\n",
        "    texts.append(row['headline'])\n",
        "    labels.append(row['is_sarcastic'])\n",
        "\n",
        "  # Closing file\n",
        "  f.close()\n",
        "\n",
        "  # Maximum length of the padded sequences\n",
        "  # Find the maximum length of texts across all texts\n",
        "  max_length = max(len(s.split()) for s in texts) #initial hardcode value : max_length = 32\n",
        "\n",
        "  # Number of examples to use for training\n",
        "  training_size = int(0.8 * len(labels)) #80% training data split\n",
        "\n",
        "  # Split the sentences\n",
        "  train_sentences = texts[0:training_size]\n",
        "  test_sentences = texts[training_size:]\n",
        "\n",
        "  # Split the labels\n",
        "  train_labels = labels[0:training_size]\n",
        "  test_labels = labels[training_size:]\n",
        "\n",
        "  # Max_words of the tokenizer\n",
        "  max_words = 10000\n",
        "\n",
        "  # Output dimensions of the Embedding layer\n",
        "  embedding_dim = 16\n",
        "\n",
        "  # Parameters for padding and OOV tokens\n",
        "  oov_tok = '<OOV>'\n",
        "  trunc_type = 'post'\n",
        "  pad_type = 'post'\n",
        "\n",
        "  # Initialize the Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)\n",
        "\n",
        "  # Generate the word index dictionary\n",
        "  tokenizer.fit_on_texts(train_sentences)\n",
        "  train_word_index = tokenizer.word_index\n",
        "\n",
        "  # Vocabulary size\n",
        "  vocab_size = len(tokenizer.word_index)+ 1\n",
        "\n",
        "  # Generate and pad the training sequences\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "  train_padded_seqs = pad_sequences(train_sequences, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n",
        "\n",
        "  # Generate and pad the testing sequences\n",
        "  test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "  test_padded_seqs = pad_sequences(test_sequences, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n",
        "\n",
        "  # Convert the labels lists into numpy arrays\n",
        "  final_train_labels = np.array(train_labels)\n",
        "  final_test_labels = np.array(test_labels)\n",
        "\n",
        "  return train_padded_seqs,final_train_labels,test_padded_seqs,final_test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMF4afx2IdHo"
      },
      "source": [
        "## Build and Compile the Model\n",
        "\n",
        "Next, you will build the model. The architecture is similar to the previous lab but you will use a [GlobalAveragePooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer instead of `Flatten` after the Embedding. This adds the task of averaging over the sequence dimension before connecting to the dense layers. See a short demo of how this works using the snippet below. Notice that it gets the average over 3 arrays (i.e. `(10 + 1 + 1) / 3` and `(2 + 3 + 1) / 3` to arrive at the final output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evlU_kqOshc4"
      },
      "source": [
        "This added computation reduces the dimensionality of the model as compared to using `Flatten()` and thus, the number of training parameters will also decrease. See the output of `model.summary()` below and see how it compares if you swap out the pooling layer with a simple `Flatten()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X734nBFUq-b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras as keras\n",
        "from hyperas.distributions import choice\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "import time\n",
        "from time import time\n",
        "from pprint import pprint\n",
        "\n",
        "def create_model(train_padded_seqs,final_train_labels,test_padded_seqs,final_test_labels):\n",
        "\n",
        "  #Initiallize the hyperparameters\n",
        "  start_time = time()\n",
        "  num_epochs = 5\n",
        "  output_dim_list = [16, 32, 64] #Output dimension parameter in first layer\n",
        "  num_units = [16, 32, 64] #no. of units in Third layers\n",
        "  lr = [10**-3, 10**-2, 10**-1] #learning rate values for optimizers\n",
        "\n",
        "  # Build the model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #First Embedding layer\n",
        "  model.add(keras.layers.Embedding(vocab_size, output_dim = {{choice([16, 32, 64])}}, input_length=max_length))\n",
        "\n",
        "  #Second layer\n",
        "  model.add(keras.layers.GlobalAveragePooling1D())\n",
        "\n",
        "  # Third layer\n",
        "  model.add(keras.layers.Dense({{choice([16, 32, 64])}}, activation='relu')),\n",
        "\n",
        "  #Fourth layer\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  #Initialize the learning rates\n",
        "  adam = Adam(learning_rate={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "  rmsprop = RMSprop(learning_rate={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "  sgd = SGD(learning_rate={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer={{choice(['adam', 'sgd', 'rmsprop'])}},\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')\n",
        "\n",
        "  # Optional to log output from Keras\n",
        "  csv_logger = keras.callbacks.CSVLogger('./dl_model.log')\n",
        "\n",
        "  result = model.fit(train_padded_seqs,final_train_labels,\n",
        "              epochs=num_epochs,\n",
        "              verbose=2,\n",
        "              validation_data=(test_padded_seqs,final_test_labels),\n",
        "              callbacks=[csv_logger])\n",
        "\n",
        "  pprint(vars(result))\n",
        "\n",
        "  # # added to collect optimisation results\n",
        "  # if 'results' not in globals():\n",
        "  #   global results\n",
        "  #   results = []\n",
        "\n",
        "  # val_acc = result.history['val_accuracy']\n",
        "  # parameters = space\n",
        "  # parameters[\"val_acc\"] = val_acc\n",
        "  # parameters[\"time\"] = str(int(time() - start_time)) + \"sec\"\n",
        "  # score, val_acc_final = model.evaluate((test_padded_seqs,final_test_labels), verbose=2)\n",
        "  # parameters[\"val_acc_final\"] = val_acc_final\n",
        "  # results.append(parameters)\n",
        "  # print(tabulate(results, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".8f\"))\n",
        "\n",
        "  #get the highest validation accuracy of the training epochs\n",
        "  validation_acc = np.amax(result.history['val_accuracy'])\n",
        "  print('Best validation acc of epoch:', validation_acc)\n",
        "\n",
        "  return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0jCJfw93mQc"
      },
      "outputs": [],
      "source": [
        "import hyperas\n",
        "from hyperas import optim\n",
        "from hyperopt import Trials,STATUS_OK, tpe\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "try:\n",
        "    best_run, best_model, space = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          # functions=['parse_json_file'],\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name=os.path.join('..','gdrive','My Drive','Colab Notebooks','C3_W2_Lab_2_sarcasm_classifier_v2'),\n",
        "                                          eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
        "                                          return_space=True,  # <-- this allows you to save the space for later evaluations\n",
        "                                          )\n",
        "except Exception as e:\n",
        "    logging.exception(\"message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrO6mGvS398w"
      },
      "source": [
        "Refer this link for details - https://www.linkedin.com/pulse/hyperas-prateek-khanna/\n",
        "\n",
        "\n",
        "Hyperas is a simple wrapper for hyperparameter optimization using keras and hyperopt. It uses template notation to define hyper-parameter ranges to tune. You can wrap the parameters you want to optimize into double curly brackets {{...}} and choose a distribution over which to run the algorithm. Hyperas translates your script into hyperopt compliant code at runtime.\n",
        "\n",
        "Hyperas needs a data function to load your data. It returns your X_train, Y_train, X_test and Y_test values.The Model Function is where you define your model. You can use all the available keras functions and layers to create the model.\n",
        "\n",
        "Hyperas provides a optim.minimize function for minimizing a keras model for given data and implicit hyperparameters. It takes as input the following parameters:\n",
        "\n",
        "**model:** A function defining a keras model with hyperas templates, which returns a valid hyperopt results dictionary, e.g. return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "**data:** A parameter-less function that defines and return all data needed in the above model definition.\n",
        "\n",
        "**algo:** A hyperopt algorithm, like tpe.suggest or rand.suggest. Tree-structured Parzen Estimator (TPE) algorithm is a bayesian algorithm which explore intelligently the search space while narrowing down to the estimated best parameters. Rand.suggest on the other hand does a random search through the search space.\n",
        "\n",
        "**max_evals:** Maximum number of optimization runs\n",
        "trials: A hyperopt Trials object, used to store intermediate results for all optimization runs\n",
        "\n",
        "**rseed:** Integer random seed for experiments\n",
        "notebook_name: If running from an ipython notebook, provide filename (not path)\n",
        "\n",
        "**verbose:** Print verbose output\n",
        "\n",
        "**eval_space:** Evaluate the best run in the search space such that 'choice's contain actually meaningful values instead of mere indices\n",
        "return_space: Return the hyperopt search space object (e.g. for further processing) as last return value\n",
        "\n",
        "**keep_temp:** Keep temp_model.py file on the filesystem\n",
        "\n",
        "**Return value:**A pair consisting of the results dictionary of the best run and the corresponding keras model. If \"return_space\" is True it also returns the hyperopt search space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4aJn5GJfegb"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train, X_test, Y_test = data()\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(X_test, Y_test))\n",
        "print(\"Best performing model chosen hyper-parameters:\")\n",
        "print(best_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfFyVMzej2pu"
      },
      "outputs": [],
      "source": [
        "best_model.save('./best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_bWhGOSJLLm"
      },
      "source": [
        "## Visualize the Results\n",
        "\n",
        "You can use the cell below to plot the training results. You may notice some overfitting because your validation accuracy is slowly dropping while the training accuracy is still going up. See if you can improve it by tweaking the hyperparameters. Some example values are shown in the lectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HYfBKXjkmU8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.fugure_format = 'retina'\n",
        "def plot_loss_acc(history):\n",
        "  #-----------------------------------------------------------\n",
        "  # Retrieve a list of list results on training and test data\n",
        "  # sets for each training epoch\n",
        "  #-----------------------------------------------------------\n",
        "  acc      = history.history[     'accuracy' ]\n",
        "  val_acc  = history.history[ 'val_accuracy' ]\n",
        "  loss     = history.history[    'loss' ]\n",
        "  val_loss = history.history['val_loss' ]\n",
        "  epochs   = range(len(acc)) # Get number of epochs\n",
        "  #------------------------------------------------\n",
        "  # Plot training and validation accuracy per epoch\n",
        "  #------------------------------------------------\n",
        "  plt.plot  ( epochs,     acc, label='Training accuracy' )\n",
        "  plt.plot  ( epochs, val_acc, label='Validation accuracy' )\n",
        "  plt.title ('Training and validation accuracy')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.figure()\n",
        "  #------------------------------------------------\n",
        "  # Plot training and validation loss per epoch\n",
        "  #------------------------------------------------\n",
        "  plt.plot  ( epochs,     loss, label='Training loss' )\n",
        "  plt.plot  ( epochs, val_loss, label='Validation loss' )\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title ('Training and validation loss'   )\n",
        "\n",
        "# Plot training results\n",
        "plot_loss_acc(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUsE0gsKexDZ"
      },
      "outputs": [],
      "source": [
        "# model.predict([\"Donald Trump is the best thing happened to US\",])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN6kaxxcJQgd"
      },
      "source": [
        "## Visualize Word Embeddings\n",
        "\n",
        "As before, you can visualize the final weights of the embeddings using the [Tensorflow Embedding Projector](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9MqihtEkzQ9"
      },
      "outputs": [],
      "source": [
        "# Get the index-word dictionary\n",
        "reverse_word_index = tokenizer.index_word\n",
        "\n",
        "# Get the embedding layer from the model (i.e. first layer)\n",
        "embedding_layer = model.layers[0]\n",
        "\n",
        "# Get the weights of the embedding layer\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
        "print(embedding_weights.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoBXVffknldU"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "# Open writeable files\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
        "for word_num in range(1, vocab_size):\n",
        "\n",
        "  # Get the word associated at the current index\n",
        "  word_name = reverse_word_index[word_num]\n",
        "\n",
        "  # Get the embedding weights associated with the current index\n",
        "  word_embedding = embedding_weights[word_num]\n",
        "\n",
        "  # Write the word name\n",
        "  out_m.write(word_name + \"\\n\")\n",
        "\n",
        "  # Write the word embedding\n",
        "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
        "\n",
        "# Close the files\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4eZ5HtVnnEE"
      },
      "outputs": [],
      "source": [
        "# Import files utilities in Colab\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "# Download the files\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GierJvdJWMt"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "In this lab, you were able to build a binary classifier to detect sarcasm. You saw some overfitting in the initial attempt and hopefully, you were able to arrive at a better set of hyperparameters.\n",
        "\n",
        "So far, you've been tokenizing datasets from scratch and you're treating the vocab size as a hyperparameter. Furthermore, you're tokenizing the texts by building a vocabulary of full words. In the next lab, you will make use of a pre-tokenized dataset that uses a vocabulary of *subwords*. For instance, instead of having a uniqe token for the word `Tensorflow`, it will instead have a token each for `Ten`, `sor`, and `flow`. You will see the motivation and implications of having this design in the next exercise. See you there!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
