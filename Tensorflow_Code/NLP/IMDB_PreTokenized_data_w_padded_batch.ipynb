{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/NLP/IMDB_PreTokenized_data_w_padded_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLKIel77CJPi"
      },
      "source": [
        "## Ungraded Lab: Subword Tokenization with the IMDB Reviews Dataset\n",
        "\n",
        "In this lab, you will look at a pre-tokenized dataset that is using subword text encoding. This is an alternative to word-based tokenization which you have been using in the previous labs. You will see how it works and its implications on preparing your data and training your model.\n",
        "\n",
        "Let's begin!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrzOn9quZ0Sv"
      },
      "source": [
        "## Download the IMDB reviews plain text and tokenized datasets\n",
        "\n",
        "First, you will download the [IMDB Reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) dataset from Tensorflow Datasets. You will get two configurations:\n",
        "\n",
        "* `plain_text` - this is the default and the one you used in Lab 1 of this week\n",
        "* `subwords8k` - a pre-tokenized dataset (i.e. instead of sentences of type string, it will already give you the tokenized sequences). You will see how this looks in later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB6tbQqKFw-j"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Download the plain text default config\n",
        "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "\n",
        "# Download the subword encoded pretokenized dataset\n",
        "imdb_subwords, info_subwords = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg7rJB2TG-cw"
      },
      "outputs": [],
      "source": [
        "#Analyzing the metadata\n",
        "print(info_plaintext)\n",
        "print(info_subwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGP3wo5jHHvU"
      },
      "outputs": [],
      "source": [
        "print(imdb_plaintext)\n",
        "\n",
        "'''Output elements:\n",
        "{Split('train'):\n",
        "    <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
        "    TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
        "Split('test'):\n",
        "    <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
        "    TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
        "Split('unsupervised'):\n",
        "    <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
        "    TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKoc8CzqHKsp"
      },
      "outputs": [],
      "source": [
        "print(imdb_subwords)\n",
        "\n",
        "'''Output elements:\n",
        "{Split('train'):\n",
        "  <_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
        "    TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
        "Split('test'):\n",
        "  <_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
        "  TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
        "Split('unsupervised'):\n",
        "  <_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
        "  TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JggMZRCEcdlN"
      },
      "source": [
        "# **Compare the two datasets**\n",
        "\n",
        "As mentioned, the data types returned by the two datasets will be different. For the default, it will be strings as you also saw in Lab 1. Notice the description of the `text` key below and the sample sentences:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plaintext dataset**"
      ],
      "metadata": {
        "id": "QVokj_CRzHj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmBgudHmIi8z"
      },
      "outputs": [],
      "source": [
        "# Print description of features\n",
        "print(info_plaintext.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04e_Hww3IvlM"
      },
      "outputs": [],
      "source": [
        "# Take 2 training examples and print the text feature\n",
        "for sample in imdb_plaintext['train'].take(1):\n",
        "  print(\"Sample Record :\",'\\n',sample)\n",
        "  print('\\n',\"Sample text part :\",'\\n',sample[0])\n",
        "  print('\\n',\"Sample label part :\",'\\n',sample[1])\n",
        "\n",
        "  '''Output sample:\n",
        "  ( <tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>,\n",
        "    <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87JvGD9dId5"
      },
      "source": [
        "## **Pre-Tokenized dataset**\n",
        "\n",
        "For `subwords8k`, the dataset is already tokenized so the data type will be integers. Notice that the `text` features also include an `encoder` field and has a `vocab_size` of around 8k, hence the name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wp_a7292mxk"
      },
      "outputs": [],
      "source": [
        "# Print description of features\n",
        "print(info_subwords.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ssDU_TddyLF"
      },
      "source": [
        "If you print the results, you will not see string sentences but a sequence of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhrd_brwMAT4"
      },
      "outputs": [],
      "source": [
        "# Take 1 training example and print its contents\n",
        "for sample in imdb_subwords['train'].take(1):\n",
        "  print(\"Sample Record :\",'\\n',sample)\n",
        "  print('\\n',\"Sample text part :\",'\\n',sample[0])\n",
        "  print('\\n',\"Sample label part :\",'\\n',sample[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decoding records in Pre-Tokenized dataset**"
      ],
      "metadata": {
        "id": "0_XnkXtuzYfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg6EQ6x2SYPV"
      },
      "outputs": [],
      "source": [
        "# Get the encoder\n",
        "tokenizer_subwords = info_subwords.features['text'].encoder\n",
        "print(tokenizer_subwords)\n",
        "\n",
        "# Take 2 training examples and decode the text feature\n",
        "for sample in imdb_subwords['train'].take(1):\n",
        "  print('\\n',\"Original token data in imdb_subwords :\",'\\n',sample[0])\n",
        "  print('\\n',\"Decoded token data using tokenizer_subwords:\",'\\n',tokenizer_subwords.decode(sample[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWOrkYGug--B"
      },
      "source": [
        "You can get the `encoder` object included in the download and use it to decode the sequences above. You'll see that you will arrive at the same sentences provided in the `plain_text` config:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20_XNWbXiwcE"
      },
      "source": [
        "*Note: The documentation for the encoder can be found [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder) but don't worry if it's marked as deprecated. As mentioned, the objective of this exercise is just to show the characteristics of subword encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKrbY2fjjFHM"
      },
      "source": [
        "# Subword Text Encoding\n",
        "\n",
        "From previous labs, the number of tokens in the sequence is the same as the number of words in the text (i.e. word tokenization). The following cells shows a review of this process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokens - Plaintext data**"
      ],
      "metadata": {
        "id": "K3czqhPzz5NA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0REvpJs6N8PB"
      },
      "outputs": [],
      "source": [
        "#Get the train set\n",
        "train_data = imdb_plaintext['train']\n",
        "\n",
        "#initialize sentences list\n",
        "train_sentences = []\n",
        "\n",
        "#Loop over all training examples and save to list\n",
        "for s,_ in train_data:\n",
        "  train_sentences.append(s.numpy().decode('utf8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0ptBjfZOk40"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 10000\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "\n",
        "# Generate the word index dictionary for the training sentences\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate the training sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNUlDp76lf94"
      },
      "source": [
        "The cell above uses a `vocab_size` of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created. See the result below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_wUfD9aQDfn"
      },
      "outputs": [],
      "source": [
        "# Decode the first sequence using the Tokenizer class\n",
        "print(train_sequences[0:2])\n",
        "tokenizer.sequences_to_texts(train_sequences[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0HQqkBmpujb"
      },
      "source": [
        "For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then the `vocab_size` will increase to more than 88k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRrpIzAZRIaN"
      },
      "outputs": [],
      "source": [
        "# Total number of words in the word index dictionary (i.e, training data)\n",
        "len(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokens - PreTokenized dataset**"
      ],
      "metadata": {
        "id": "xCKtNNM60v4V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McxNKhHIsNvl"
      },
      "source": [
        "*Subword text encoding* gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAeyxk5oR52E"
      },
      "outputs": [],
      "source": [
        "#print the subwords in imdb_reviews/subwords8k data\n",
        "print(tokenizer_subwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqyMSZbnwFBo"
      },
      "outputs": [],
      "source": [
        "# Print the subwords\n",
        "print(tokenizer_subwords.subwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OOV problem in Plaintext dataset**"
      ],
      "metadata": {
        "id": "V0JgB_9e2CQK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaRA9LBUwfHM"
      },
      "source": [
        "If you use it on the previous plain text sentence, you'll see that it won't have any OOVs even if it has a smaller vocab size (only 8k compared to 10k above):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH3sxEdCXmy3"
      },
      "outputs": [],
      "source": [
        "# Encode the first plaintext sentence using the subword text encoder\n",
        "tokenized_plaintext = tokenizer_subwords.encode(train_sentences[0])\n",
        "print(\"Sentence :\",train_sentences[0],'\\n')\n",
        "print(\"Tokenized Sentence (using subword text encoder):\",tokenized_plaintext, '\\n')\n",
        "\n",
        "# Decode the sequence\n",
        "original_plaintext = tokenizer_subwords.decode(tokenized_plaintext)\n",
        "print(\"Decoded Sentence :\",original_plaintext,'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL9O3hEqw4Bl"
      },
      "source": [
        "Subword encoding can even perform well on words that are not commonly found on movie reviews. See first the result when using the plain text tokenizer. As expected, it will show many OOVs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHRj1J0j8ApE"
      },
      "outputs": [],
      "source": [
        "# Define sample sentence\n",
        "sample_string = 'TensorFlow, from basics to mastery'\n",
        "\n",
        "# Encode using the plain text tokenizer\n",
        "tokenized_string = tokenizer.texts_to_sequences([sample_string])\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "# Decode and print the result\n",
        "original_string = tokenizer.sequences_to_texts(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhQ-4O-uxdbJ"
      },
      "source": [
        "Then compare to the subword text encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPl2BXhYEHRP"
      },
      "outputs": [],
      "source": [
        "# Encode using the subword text encoder\n",
        "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "# Decode and print the results\n",
        "original_string = tokenizer_subwords.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89sbfXjz0MSW"
      },
      "source": [
        "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using word-encoding, you ended up with 11 tokens instead. The mapping for this sentence is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3t7vvNLEZml"
      },
      "outputs": [],
      "source": [
        "# Show token to subword mapping:\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ueIj_XcBhN"
      },
      "outputs": [],
      "source": [
        "# Show token to subword mapping:\n",
        "for ts in tokenized_plaintext:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ22ugch1TFy"
      },
      "source": [
        "# **Data Preparation**\n",
        "\n",
        "You will now train your model using this pre-tokenized dataset. Since these are already saved as sequences, you can jump straight to making uniform sized arrays for the train and test sets. These are also saved as `tf.data.Dataset` type so you can use the [`padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) method to create batches and pad the arrays into a uniform size for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecK1HIZdcdeZ"
      },
      "outputs": [],
      "source": [
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "\n",
        "#Get train and test splits\n",
        "train_data, test_data = imdb_subwords['train'], imdb_subwords['test']\n",
        "\n",
        "#Shuffle the training data\n",
        "train_data_shuffled = train_data.shuffle(buffer_size)\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "padded_train_data_shuffled = train_data_shuffled.padded_batch(batch_size)\n",
        "\n",
        "#Test data pre-processing\n",
        "padded_test_data = test_data.padded_batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCjHCG7s2sAR"
      },
      "source": [
        "# **Building the model**\n",
        "\n",
        "Next, you will build the model. You can just use the architecture from the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGd57CvndP5i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define dimensionality of the embedding\n",
        "embedding_dim = 64\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size,embedding_dim), #notice the input dimension is tokenizer_subwords.vocab_size\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aOn2bAc3AUj"
      },
      "source": [
        "Similarly, you can use the same parameters for training. In Colab, it will take around 20 seconds per epoch (without an accelerator) and you will reach around 94% training accuracy and 88% validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkt8c5dNuUlT"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "history = model.fit(padded_train_data_shuffled, epochs=num_epochs, verbose=2,\n",
        "                    validation_data=padded_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ygYaD6H3qGX"
      },
      "source": [
        "## Visualize the results\n",
        "\n",
        "You can use the cell below to plot the training results. See if you can improve it by tweaking the parameters such as the size of the embedding and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_rMnm7WxQGT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "# Plot the accuracy and results\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0TRE-Lb4C5b"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "In this lab, you saw how subword text encoding can be a robust technique to avoid out-of-vocabulary tokens. It can decode uncommon words it hasn't seen before even with a relatively small vocab size. Consequently, it results in longer token sequences when compared to full word tokenization. Next week, you will look at other architectures that you can use when building your classifier. These will be recurrent neural networks and convolutional neural networks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}