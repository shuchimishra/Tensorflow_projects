{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_L7ssW085fTH5moj8yShrvnuJYpQ2OgT",
      "authorship_tag": "ABX9TyOh4AnFBGQLAenSOi4o6uuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/NLP/Category4_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPKEvo-JhHdY"
      },
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prints loss and accuracy curves. You can indicate first epoch to plot.\n",
        "def plot_loss_acc(history, firstepoch=0):\n",
        "  '''Plots the training and validation loss and accuracy from a history object'''\n",
        "  acc = history.history['accuracy']\n",
        "  acc = acc[firstepoch:]\n",
        "  val_acc = history.history['val_accuracy']\n",
        "\n",
        "  val_acc = val_acc[firstepoch:]\n",
        "  loss = history.history['loss']\n",
        "  loss=loss[firstepoch:]\n",
        "  val_loss = history.history['val_loss']\n",
        "  val_loss = val_loss[firstepoch:]\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'go-', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "  plt.plot(epochs, val_loss, 'go-', label='Validation Loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "B0HgkWVelqmw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PhV1drfdi7w_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ5bGntoiA36",
        "outputId": "da59a276-27b5-4e0f-d43e-fdca55e297cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sarcasm.json', <http.client.HTTPMessage at 0x78df31773190>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"./sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "# Initialize the lists\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Collect sentences and labels into the lists\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])"
      ],
      "metadata": {
        "id": "R2JNFmW6iSwz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "metadata": {
        "id": "Tqp9yEoeeokM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_size = 20000\n",
        "\n",
        "# Split the sentences\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "\n",
        "# Split the labels\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "metadata": {
        "id": "qrlDDB-Cis52"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(glove_url, 'glove.6B.100d.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwXxse93myhH",
        "outputId": "664742a9-35cd-456a-8a2c-acab866b3a62"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('glove.6B.100d.txt', <http.client.HTTPMessage at 0x78df31773fa0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(glove_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF9OcfO3phqj",
        "outputId": "022ad5f1-023c-46ec-8524-d93185ad6ad6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/tmp/tmp9y6k7pig', <http.client.HTTPMessage at 0x78df30820160>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ldL2mfqqEK",
        "outputId": "8f5e673c-4c3e-4e2f-ca9d-a1be28709658"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-01 10:19:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-05-01 10:19:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-05-01 10:19:42--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.09MB/s    in 2m 45s  \n",
            "\n",
            "2024-05-01 10:22:27 (4.99 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft0C8xAQri2T",
        "outputId": "cce73657-4746-4588-a496-0469e8c1a60d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_FILE = './glove.6B.100d.txt'"
      ],
      "metadata": {
        "id": "1b8QVd06eXcS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit_tokenizer\n",
        "def fit_tokenizer(train_sentences, oov_token):\n",
        "    \"\"\"\n",
        "    Instantiates the Tokenizer class on the training sentences\n",
        "\n",
        "    Args:\n",
        "        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n",
        "        oov_token (string) - symbol for the out-of-vocabulary token\n",
        "\n",
        "    Returns:\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "\n",
        "    # Instantiate the Tokenizer class, passing in the correct values for oov_token\n",
        "    tokenizer = Tokenizer(oov_token=oov_token)\n",
        "\n",
        "    # Fit the tokenizer to the training sentences\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "JJQuu7Bt-OgP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "tokenizer = fit_tokenizer(training_sentences, oov_tok)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index)\n",
        "\n",
        "print(f\"Vocabulary contains {VOCAB_SIZE} unique words\\n\")"
      ],
      "metadata": {
        "id": "VoLQ24IJ-Rx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89be0127-3647-4b9b-ad5a-cf0db5df32fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 25637 unique words\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq_pad_and_trunc\n",
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "    \"\"\"\n",
        "    Generates an array of token sequences and pads them to the same length\n",
        "\n",
        "    Args:\n",
        "        sentences (list of string): list of sentences to tokenize and pad\n",
        "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
        "        padding (string): type of padding to use\n",
        "        truncating (string): type of truncating to use\n",
        "        maxlen (int): maximum length of the token sequence\n",
        "\n",
        "    Returns:\n",
        "        pad_trunc_sequences (array of int): tokenized sentences padded to the same length\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "\n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "    # #Override MAXLEN variable if necessary\n",
        "    # mlen = max([len(l) for l in sequences]) #maximum length of sequence\n",
        "\n",
        "    # #int(math.ceil(x / 10.0)) * 10 will round off to closest 10 eg. 46-->50, 23-->30\n",
        "    # mlen = int(math.ceil(mlen / 10.0)) * 10\n",
        "    # if maxlen < mlen:\n",
        "    #   maxlen = mlen\n",
        "\n",
        "    # Pad the sequences using the correct padding, truncating and maxlen\n",
        "    pad_trunc_sequences = pad_sequences(sequences,padding=padding,truncating=truncating,maxlen=maxlen)\n",
        "\n",
        "    # #int(math.ceil(x / 10.0)) * 10 will round off to closest 10 eg. 46-->50, 23-->30\n",
        "    # pad_trunc_sequences = pad_sequences(sequences,padding=padding,truncating=truncating,maxlen=maxlen)\n",
        "\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return pad_trunc_sequences"
      ],
      "metadata": {
        "id": "6vltN3ej-V6g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test your function\n",
        "train_pad_trunc_seq = seq_pad_and_trunc(training_sentences, tokenizer, padding_type, trunc_type, max_length)\n",
        "val_pad_trunc_seq = seq_pad_and_trunc(testing_sentences, tokenizer, padding_type, trunc_type, max_length)\n",
        "\n",
        "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
        "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKdDw0Ohm4k",
        "outputId": "b1ac6c77-3df2-44e1-e0c6-f1a094cf529e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded and truncated training sequences have shape: (20000, 120)\n",
            "\n",
            "Padded and truncated validation sequences have shape: (6709, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = np.array(training_labels)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "Hz__ml7ktL2Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "# with io.open(GLOVE_FILE, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ],
      "metadata": {
        "id": "h3TGnHLp-lx-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "jbyfeZWLsuVE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty numpy array with the appropriate size\n",
        "'''\n",
        "1. We need len(word_index)+1 to accomodate out of vocab token in GLOVE_EMBEDDINGS.\n",
        "2. The word vector of GLOVE_EMBEDDINGS has dimension of 100 .\n",
        "3. So size of EMBEDDINGS_MATRIX will be (len(word_index)+1, word vector of GLOVE_EMBEDDINGS size)\n",
        "'''\n",
        "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
        "\n",
        "# Iterate all of the words in the vocabulary and if the vector representation for\n",
        "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    EMBEDDINGS_MATRIX[i] = embedding_vector"
      ],
      "metadata": {
        "id": "lPsNriPC-orb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_size_1=64\n",
        "lstm_size_2=64\n",
        "dense_dim=64\n",
        "NUM_EPOCHS = 15"
      ],
      "metadata": {
        "id": "uWPUny7zlGbx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition with Conv1D\n",
        "model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embedding_dim,\n",
        "                              input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size_1, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size_2)),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Set the training parameters\n",
        "model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_lstm.summary()\n",
        "\n",
        "# Train the model\n",
        "history_lstm = model_lstm.fit(train_pad_trunc_seq, training_labels, epochs=NUM_EPOCHS, validation_data=(val_pad_trunc_seq, testing_labels))"
      ],
      "metadata": {
        "id": "ZBj_r70HeXcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_acc(historytl0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "eecf8661-28c2-4249-c9d6-6292be5aa656",
        "id": "dcCvdlRaeXcU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plot_loss_acc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0708346e6c82>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorytl0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_loss_acc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#just clean the session recomendable if we execute some times the model.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#This callback saves the best model based in val_accuracy\n",
        "BestModel = tf.keras.callbacks.ModelCheckpoint('modelDA.h5',\n",
        "                                           mode='max', monitor='val_accuracy',\n",
        "                                           verbose=1,\n",
        "                                           save_best_only=True)\n",
        "\n",
        "ES = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "IyKubctpi33f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00quUwRr2Q8m",
        "outputId": "f5eb6dc6-14b3-4bff-8cc9-a2739075467a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modeltl0.save(\"mymodel.h5\")\n",
        "modeltl0.save(\"/content/drive/MyDrive/Data Science & Machine Learning/Tensorflow Certification/Repository/exam/4/mymodel.h5\")"
      ],
      "metadata": {
        "id": "7PUYkT1Lne_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Data Science & Machine Learning/Tensorflow Certification/Repository/exam/4/mymodel.h5\" \"/Users/shuchimishra/Library/Caches/JetBrains/PyCharm2023.3/certification/Category4/mymodel-1.h5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ric7NKCo5OPf",
        "outputId": "33a1e868-d135-4275-a190-c67356d40216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/Users/shuchimishra/Library/Caches/JetBrains/PyCharm2023.3/certification/Category4/mymodel-1.h5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modeltl0.save(\"mymodel.h5\")\n",
        "# Import files utilities in Colab\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "# Download the files\n",
        "else:\n",
        "  files.download('mymodel.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WhuJkiaznt_o",
        "outputId": "fe6e2501-56b3-4a9f-f274-53c9e843963b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_90ec3503-8d58-4dbb-858a-3316c8646cc9\", \"mymodel.h5\", 1028314424)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "is-2qMWy0ez5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
