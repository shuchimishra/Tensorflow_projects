{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuchimishra/Tensorflow_projects/blob/main/Tensorflow_Code/NLP/Irish_lyrics_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OX06wagqRqG"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W4/ungraded_labs/C3_W4_Lab_2_irish_lyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqIxQYm8h06Z"
      },
      "source": [
        "# Ungraded Lab: Generating Text from Irish Lyrics\n",
        "\n",
        "In the previous lab, you trained a model on just a single song. You might have found that the output text can quickly become gibberish or repetitive. Even if you tweak the hyperparameters, the model will still be limited by its vocabulary of only 263 words. The model will be more flexible if you train it on a much larger corpus and that's what you'll be doing in this lab. You will use lyrics from more Irish songs then see how the generated text looks like. You will also see how this impacts the process from data preparation to model training. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb1mfOvch4Sv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmBFI788pOXx"
      },
      "source": [
        "## Building the Word Vocabulary\n",
        "\n",
        "You will first download the lyrics dataset. These will be from a compilation of traditional Irish songs and you can see them [here](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W4/misc/Laurences_generated_poetry.txt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xbXMcogqRqJ"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==5.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylt5qZYsWPh"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!gdown --id 15UqmiIm0xwh9mt0IYq2z3jHaauxQSTQT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v6JYQGNPXCW"
      },
      "source": [
        "Next, you will lowercase and split the plain text into a list of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "\n",
        "data = open('./irish-lyrics-eof.txt').read()\n",
        "\n",
        "#Lowercase and split the data\n",
        "corpus = data.lower().split('\\n')\n",
        "\n",
        "#preview the results\n",
        "print(corpus[0:5])"
      ],
      "metadata": {
        "id": "MwndLjMQrUgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkP2CP0qP8RD"
      },
      "source": [
        "From here, you can initialize the `Tokenizer` class and generate the word index dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Define the total words. You add 1 for the index `0` which is just the padding token.\n",
        "word_count = len(word_index)+1\n",
        "\n",
        "print(\"Word Index dictionary :\",word_index)\n",
        "print(\"Word Index Dict count :\",word_count)"
      ],
      "metadata": {
        "id": "gvzoWi-aBV66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK29FzZ7QW-4"
      },
      "source": [
        "## Preprocessing the Dataset\n",
        "\n",
        "Next, you will generate the inputs and labels for your model. The process will be identical to the previous lab. The `xs` or inputs to the model will be padded sequences, while the `ys` or labels are one-hot encoded arrays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the sequences list\n",
        "input_sequences = []\n",
        "\n",
        "# Loop over every line\n",
        "for line in corpus:\n",
        "\n",
        "    # Tokenize the current line\n",
        "    \"\"\"Note:\n",
        "    texts_to_sequences fn expects data to be list;\n",
        "    output should be list (not NESTED list)\n",
        "    \"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Loop over the line several times to generate the subphrases\n",
        "    for index in range(1, len(sequences)):\n",
        "\n",
        "        # Generate the subphrase\n",
        "        n_gram_sequences = sequences[: index + 1]\n",
        "\n",
        "        # Append the subphrase to the sequences list\n",
        "        input_sequences.append(n_gram_sequences)\n",
        "\n",
        "# Get the length of the longest line\n",
        "max_seq_length = max([len(seq) for seq in input_sequences])\n",
        "\n",
        "# Pad all sequences\n",
        "pad_seq = pad_sequences(input_sequences, maxlen=max_seq_length, padding=\"pre\")\n"
      ],
      "metadata": {
        "id": "DJjHCMQyCzQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create inputs and label by splitting the last token in the subphrases\n",
        "'''\n",
        "Note: #This is list of lists [[a][b][c]] so double parsing needed\n",
        "'''\n",
        "xs=pad_seq[:,:-1]\n",
        "ys=pad_seq[:,-1]\n",
        "\n",
        "# Convert the label into one-hot arrays\n",
        "one_hot_encoded_label = tf.keras.utils.to_categorical(ys,\n",
        "                                                        num_classes=word_count)"
      ],
      "metadata": {
        "id": "q8_8-nqMJIJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWHCO0dQGlZ"
      },
      "source": [
        "You can then print some of the examples as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJtwVB2NbOAP"
      },
      "outputs": [],
      "source": [
        "# Get sample sentence\n",
        "sentence = corpus[0].split()\n",
        "print(f'sample sentence: {sentence}')\n",
        "\n",
        "# Initialize token list\n",
        "token_list = []\n",
        "\n",
        "# Look up the indices of each word and append to the list\n",
        "for word in sentence:\n",
        "  token_list.append(tokenizer.word_index[word])\n",
        "\n",
        "# Print the token list\n",
        "print(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pick element\n",
        "elem_number = 5\n",
        "\n",
        "#Print the list and phase\n",
        "'''\n",
        "Note: Expects NESTED list as input\n",
        "'''\n",
        "print(\"Token list : \",xs[elem_number])\n",
        "print(\"Decoded text :\", tokenizer.sequences_to_texts([xs[elem_number]]))\n",
        "\n",
        "# Print label\n",
        "print(\"Corresponding one hot label : \",one_hot_encoded_label[elem_number])\n",
        "print(\"Corresponding decoded label : \",np.argmax(one_hot_encoded_label[elem_number]))"
      ],
      "metadata": {
        "id": "ytFnm4juPGGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMr6kKfzROlW"
      },
      "outputs": [],
      "source": [
        "#Pick element\n",
        "elem_number = 4\n",
        "\n",
        "#Print the list and phase\n",
        "'''\n",
        "Note: Expects NESTED list as input\n",
        "'''\n",
        "print(\"Token list : \",xs[elem_number])\n",
        "print(\"Decoded text :\", tokenizer.sequences_to_texts([xs[elem_number]]))\n",
        "\n",
        "# Print label\n",
        "print(\"Corresponding one hot label : \",one_hot_encoded_label[elem_number])\n",
        "print(\"Corresponding decoded label : \",np.argmax(one_hot_encoded_label[elem_number]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWWUZm5VPG9"
      },
      "source": [
        "## Build and compile the Model\n",
        "\n",
        "Next, you will build and compile the model. We placed some of the hyperparameters at the top of the code cell so you can easily tweak it later if you want."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "#Hyperparameter\n",
        "embedding_dim = 100\n",
        "lstm_size = 150\n",
        "lr = 0.01\n",
        "\n",
        "#Build the model\n",
        "model = Sequential([\n",
        "    layers.Embedding(input_dim=word_count,\n",
        "                     output_dim=embedding_dim,\n",
        "                     input_length=max_seq_length-1),\n",
        "    layers.Bidirectional(layers.LSTM(lstm_size)),\n",
        "    layers.Dense(word_count, activation='softmax')\n",
        "])\n",
        "\n",
        "# Use categorical crossentropy because this is a multi-class problem\n",
        "\n",
        "adam_fn = Adam(learning_rate=lr)\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    # optimizer=Adam(learning_rate=learning_rate),\n",
        "    optimizer=adam_fn,\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "edZaUE74Qwh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpI0d9cfR43c"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "From the model summary above, you'll notice that the number of trainable params is much larger than the one in the previous lab. Consequently, that usually means a slower training time. It will take roughly 7 seconds per epoch with the GPU enabled in Colab and you'll reach around 76% accuracy after 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc4zC7C4jJpN"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(xs, one_hot_encoded_label, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAzLnLATFts"
      },
      "source": [
        "You can visualize the accuracy below to see how it fluctuates as the training progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXGelKThoTT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "# Visualize the accuracy\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxKIcvGTUnw"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "Now you can let the model make its own songs or poetry! Because it is trained on a much larger corpus, the results below should contain less repetitions as before. The code below picks the next word based on the highest probability output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vc6PHgxa6Hm"
      },
      "outputs": [],
      "source": [
        "# Define seed text\n",
        "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
        "\n",
        "# Define total words to predict\n",
        "next_words = 100\n",
        "\n",
        "# Loop until desired length is reached\n",
        "for _ in range(next_words):\n",
        "\n",
        "\t# Convert the seed text to a token sequence\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "\t# Pad the sequence\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
        "\n",
        "\t# Feed to the model and get the probabilities for each index\n",
        "\tprobabilities = model.predict(token_list, verbose=0)\n",
        "\n",
        "\t# Get the index with the highest probability\n",
        "\tpredicted = np.argmax(probabilities, axis=-1)[0]\n",
        "\n",
        "\t# Ignore if index is 0 because that is just the padding.\n",
        "\tif predicted != 0:\n",
        "\n",
        "\t\t# Look up the word associated with the index.\n",
        "\t\toutput_word = tokenizer.index_word[predicted]\n",
        "\n",
        "\t\t# Combine with the seed text\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\n",
        "# Print the result\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHtrtAFAT6tn"
      },
      "source": [
        "Here again is the code that gets the top 3 predictions and picks one at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJfzKm-8mVKD"
      },
      "outputs": [],
      "source": [
        "# Define seed text\n",
        "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
        "\n",
        "# Define total words to predict\n",
        "next_words = 100\n",
        "\n",
        "# Loop until desired length is reached\n",
        "for _ in range(next_words):\n",
        "\n",
        "\t# Convert the seed text to a token sequence\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "\t# Pad the sequence\n",
        "  token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
        "\n",
        "\t# Feed to the model and get the probabilities for each index\n",
        "  probabilities = model.predict(token_list, verbose=0)\n",
        "\n",
        "  # Pick a random number from [1,2,3]\n",
        "  choice = np.random.choice([1,2,3])\n",
        "\n",
        "  # Sort the probabilities in ascending order\n",
        "  # and get the random choice from the end of the array\n",
        "  predicted = np.argsort(probabilities)[0][-choice]\n",
        "\n",
        "\t# Ignore if index is 0 because that is just the padding.\n",
        "  if predicted != 0:\n",
        "\n",
        "\t\t# Look up the word associated with the index.\n",
        "\t  output_word = tokenizer.index_word[predicted]\n",
        "\n",
        "\t\t# Combine with the seed text\n",
        "\t  seed_text += \" \" + output_word\n",
        "\n",
        "# Print the result\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0--sdMUJ_k"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "This lab shows the effect of having a larger dataset to train your text generation model. As expected, this will take a longer time to prepare and train but the output will less likely become repetitive or gibberish. Try to tweak the hyperparameters and see if you get better results. You can also find some other text datasets and use it to train the model here.  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}